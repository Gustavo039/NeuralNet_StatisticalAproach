[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Redes Neurais Artificiais: um bom lugar para um estatístico se deitar",
    "section": "",
    "text": "Prefácio\nRedes Neurais Artificiais: um bom lugar para um estatístico se deitar\nNos últimos anos, a explosão no volume e na variedade de dados disponíveis tem transformado a forma como enfrentamos desafios analíticos. Este cenário dinâmico tem impulsionado avanços significativos em áreas como a Ciência de Dados, Estatística e Inteligência Artificial, com o desenvolvimento de modelos mais flexíveis e robustos, capazes de lidar com a complexidade crescente dos dados modernos. Modelos que possam lidar com cenários onde o número de variáveis supera o de observações, sem sacrificar a simplicidade e interpretabilidade, tornaram-se cada vez mais essenciais.\nDentro desse contexto, as redes neurais artificiais emergem como uma poderosa metodologia. Apesar de ser um conceito introduzido há décadas no campo da Inteligência Artificial, somente nos últimos anos as redes neurais ganharam uma popularidade generalizada, devido ao seu sucesso em diversas aplicações práticas. Contudo, mesmo com sua ampla aplicabilidade, as redes neurais ainda não são amplamente exploradas em cursos de graduação em Estatística. A terminologia frequentemente associada ao aprendizado de máquinas e a complexidade algorítmica envolvida podem representar barreiras significativas para estatísticos em formação.\nEste livro surge como uma resposta a essa lacuna, oferecendo uma introdução acessível e orientada à modelagem de redes neurais sob uma ótica estatística. Com foco em apresentar os conceitos fundamentais de forma clara e direta, este material busca facilitar o aprendizado para aqueles que já possuem uma base sólida em Estatística, mas que encontram dificuldades ao transitar para áreas como o Aprendizado de Máquinas. Além disso, o livro se complementa com rotinas implementadas em R, permitindo uma aplicação prática e concreta dos modelos discutidos.\nEstudos de simulação também serão explorados para avaliar a performance dos estimadores sob diferentes configurações, tais como funções de ativação, número de camadas ocultas (hidden layers), tamanho da amostra e função de perda — um componente crítico no processo de otimização e estimação via backpropagation. A combinação dessas abordagens permitirá uma compreensão mais profunda e adaptada ao público estatístico, trazendo luz às potencialidades e limitações das redes neurais artificiais em cenários reais."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "GD.html",
    "href": "GD.html",
    "title": "1  Regressão Linear e Gradiente Descendente",
    "section": "",
    "text": "2 Regressão Linear\nA regressão linear simples é um método estatístico utilizado para modelar a relação entre duas variáveis: uma variável dependente (ou resposta), que desejamos prever, e uma variável independente (ou explicativa), que usamos para fazer essa previsão. O objetivo da regressão linear é encontrar uma função linear que melhor descreva essa relação.\nPor exemplo: podemos utilizar o número de banheiros e quartos de uma casa para predizer seu valor, o número de gols de um atacante utilizando seu histórico dos últimos anos e sua idade ou ainda em um cenário atual, muitas startups que desenvolvem tecnologias de inteligência artificial estão usando regressão linear para prever receitas futuras com base em métricas como usuários ativos, engajamento em plataformas digitais e número de clientes corporativos.\nDentro de uma visão matemática e estatística, um modelo de regressão linear possui uma das estruturas mais simples, dado por:\n\\[Y = \\beta_0 +\\beta_1x_1 + \\epsilon\\]\nonde\n\n\\(Y\\) é a variável resposta\n\\(\\beta_0\\) é o intercepto\n\\(\\beta_1\\) é o coeficiente que pondera a variável explicativa \\(x_1\\)\n\\(epsilon\\) é o erro aleatório associado ao modelo. Veremos mais pra frente suas pressuposições e como ele se torna um dos elementos principais para a construção de um modelo lienar\n\nA essência da regressão linear simples está na tentativa de ajustar uma linha reta aos dados que minimiza a diferença entre os valores observados de \\(Y\\) e os valores preditos pela linha, utilizando para isso as variáveis explicativas\n\n\n3 Motivação\nDentre o contexto de regressão linear, um conjunto de variáveis pode ser utilizada para predizer o valor de uma outra variável. O processo de escolha de um conjunto de variáveis explicativas que melhor predizem a variável resposta é chamado de modelagem.\nA diferença entre o valor predito e o valor verdadeiro da variável de estudo é chamado de resíduo.\nUma das formas de estimação de um modelo linear é minimizando o o erro total do modelo, ou seja, encontrando o modelo que minimza o valor do resíduo.\nDe maneira detalhada, deseja-se estimar o modelo que minimiza a soma dos resíduos ao quadrado\n\\[SRQ = \\sum^n_{i=0} (y_i - \\hat y_i)^2\\]\nA SRQ pode ser chamada de uma função de custo.\nA minimização entra na área de otimização matematica em otimização.\nExistem diversos métodos\n\nMínimos Quadrados\nFunção de Verossimilhança\nGradiente Descendente\n\n\n\n4 Função de custo\n\n\n5 Cálculos\nO gradiente em relação em relação aos pesos é dado por:\n\\[D_m = \\frac{\\delta(Funcao de Custo)}{\\delta m } = \\frac{\\delta}{\\delta m}(\\frac{1}{n}\\sum^n_{i=0}(y_i - \\hat y)^2)\\]\n\\[D_m = \\frac{2}{n}(\\sum (y_i- \\hat y_i) \\times \\frac{\\delta}{\\delta m}(y_i-\\hat y_i))\\]\n\\[D_m = \\frac{2}{n}(\\sum (y_i- \\hat y_i) \\times \\frac{\\delta}{\\delta m}(y_i - (mx_i + c)))\\]\n\\[D_m = \\frac{2}{n}(\\sum (y_i- \\hat y_i) \\times(-x_i))\\]\n\\[D_m = -\\frac{2}{n}(\\sum x_i(y_i- \\hat y_i))\\]\nAssim os gradientes sáo dados por\n\\[D_M = -\\frac{1}{n}(\\sum x_i (y - \\hat y_i))\\] e\n\\[D_C = -\\frac{1}{n}(\\sum(y_i - \\hat y_i))\\]\n\n\n6 Variações\n::: {.content-visible when-format=“html”}\n\n\n\n\n\nFigure 6.1: Animation of a tesseract, a cube changing over time.\n\n\n\n\n7 Mínimos Quadrados x Gradiente Descendente\nUma pergunta comum no contexto de regressão linear é: Por que utilizar um método iterativo quando já conhecemos uma fórmula direta para calcular os coeficientes?\nA resposta principal está na formulação em forma fechada da solução, dada por:\n\\[\\beta = (X^TX)^{-1}X^TY\\]\nonde\n\n\\(X\\) é a matriz de planejamento, contendo os valores das variáveis explicativas (ou preditoras);\n\\(Y\\) é o vetor coluna que representa os valores da variável resposta (ou dependente).\n\n\nInversão de Matriz\n\nA primeira parte da fórmula envolve a inversão da matriz \\((X^TX)\\). Embora a inversão de uma matriz seja possível em teoria, em termos computacionais ela pode ser extremamente custosa. O processo de inversão tem um custo computacional que aumenta exponencialmente com o tamanho da matriz, especialmente à medida que o número de variáveis explicativas (ou colunas de X) aumenta.\nO custo de inverter uma matriz \\(n \\times n\\) é aproximadamente \\(O(N^3)\\) o que significa que, para dados com muitas variáveis, o tempo de execução cresce de maneira cúbica.\nAlém disso, se a matriz \\(X^TX\\) for mal-condicionada (ou seja, quando algumas variáveis são altamente correlacionadas entre si), o processo de inversão pode se tornar instável, resultando em erros numéricos.\n\nEficiência Computacional e Métodos Iterativos\n\nDevido ao elevado custo de calcular diretamente a inversa dE \\(X^TX\\) métodos iterativos, como os baseados em gradiente, são frequentemente preferidos em cenários com grandes conjuntos de dados ou alta dimensionalidade. Esses métodos não exigem a inversão direta da matriz e podem fornecer aproximações suficientemente boas para os coeficientes \\(\\beta\\) com um custo computacional muito menor.\n\n\n8 Funções de custo baseados na Verossimilhança\n\n\n9 GD - Caso Logístico\nA regressão logística é utilizada quando o desejamos classficar alguma classe. É um método pertencente a classse dos MLGs e possui certes diferenças para a aplaicação do me´todo de Gradiente Descendente\nA regressão logística é dada pela seguinte função\n$$"
  }
]