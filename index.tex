% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Redes Neurais Artificiais: um bom lugar para um estatístico se deitar},
  pdfauthor={Gustavo Almeida Silva e Tiago Maia Magalhães},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Redes Neurais Artificiais: um bom lugar para um estatístico se
deitar}
\author{Gustavo Almeida Silva e Tiago Maia Magalhães}
\date{Invalid Date}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[enhanced, breakable, boxrule=0pt, interior hidden, frame hidden, borderline west={3pt}{0pt}{shadecolor}, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{prefuxe1cio}{%
\chapter*{Prefácio}\label{prefuxe1cio}}
\addcontentsline{toc}{chapter}{Prefácio}

\markboth{Prefácio}{Prefácio}

Redes Neurais Artificiais: um bom lugar para um estatístico se deitar

Nos últimos anos, a explosão no volume e na variedade de dados
disponíveis tem transformado a forma como enfrentamos desafios
analíticos. Este cenário dinâmico tem impulsionado avanços
significativos em áreas como a Ciência de Dados, Estatística e
Inteligência Artificial, com o desenvolvimento de modelos mais flexíveis
e robustos, capazes de lidar com a complexidade crescente dos dados
modernos. Modelos que possam lidar com cenários onde o número de
variáveis supera o de observações, sem sacrificar a simplicidade e
interpretabilidade, tornaram-se cada vez mais essenciais.

Dentro desse contexto, as redes neurais artificiais emergem como uma
poderosa metodologia. Apesar de ser um conceito introduzido há décadas
no campo da Inteligência Artificial, somente nos últimos anos as redes
neurais ganharam uma popularidade generalizada, devido ao seu sucesso em
diversas aplicações práticas. Contudo, mesmo com sua ampla
aplicabilidade, as redes neurais ainda não são amplamente exploradas em
cursos de graduação em Estatística. A terminologia frequentemente
associada ao aprendizado de máquinas e a complexidade algorítmica
envolvida podem representar barreiras significativas para estatísticos
em formação.

Este livro surge como uma resposta a essa lacuna, oferecendo uma
introdução acessível e orientada à modelagem de redes neurais sob uma
ótica estatística. Com foco em apresentar os conceitos fundamentais de
forma clara e direta, este material busca facilitar o aprendizado para
aqueles que já possuem uma base sólida em Estatística, mas que encontram
dificuldades ao transitar para áreas como o Aprendizado de Máquinas.
Além disso, o livro se complementa com rotinas implementadas em R,
permitindo uma aplicação prática e concreta dos modelos discutidos.

Estudos de simulação também serão explorados para avaliar a performance
dos estimadores sob diferentes configurações, tais como funções de
ativação, número de camadas ocultas (hidden layers), tamanho da amostra
e função de perda --- um componente crítico no processo de otimização e
estimação via backpropagation. A combinação dessas abordagens permitirá
uma compreensão mais profunda e adaptada ao público estatístico,
trazendo luz às potencialidades e limitações das redes neurais
artificiais em cenários reais.

\bookmarksetup{startatroot}

\hypertarget{regressuxe3o-linear-e-gradiente-descendente}{%
\chapter{Regressão Linear e Gradiente
Descendente}\label{regressuxe3o-linear-e-gradiente-descendente}}

A regressão linear simples é um método estatístico utilizado para
modelar a relação entre duas variáveis: uma variável dependente (ou
resposta), que desejamos prever, e uma variável independente (ou
explicativa), que usamos para fazer essa previsão. O objetivo da
regressão linear é encontrar uma função linear que melhor descreva essa
relação.

Por exemplo: podemos utilizar o número de banheiros e quartos de uma
casa para predizer seu valor, o número de gols de um atacante utilizando
seu histórico dos últimos anos e sua idade ou ainda em um cenário atual,
muitas startups que desenvolvem tecnologias de inteligência artificial
estão usando regressão linear para prever receitas futuras com base em
métricas como usuários ativos, engajamento em plataformas digitais e
número de clientes corporativos.

Dentro de uma visão matemática e estatística, um modelo de regressão
linear possui uma das estruturas mais simples, dado por:

\[Y = \beta_0 +\beta_1x_1 + \epsilon\]

onde

\begin{itemize}
\item
  \(Y\) é a variável resposta
\item
  \(\beta_0\) é o intercepto
\item
  \(\beta_1\) é o coeficiente que pondera a variável explicativa \(x_1\)
\item
  \(epsilon\) é o erro aleatório associado ao modelo. Veremos mais pra
  frente suas pressuposições e como ele se torna um dos elementos
  principais para a construção de um modelo lienar
\end{itemize}

A essência da regressão linear simples está na tentativa de ajustar uma
linha reta aos dados que minimiza a diferença entre os valores
observados de \(Y\) e os valores preditos pela linha, utilizando para
isso as variáveis explicativas

\bookmarksetup{startatroot}

\hypertarget{motivauxe7uxe3o}{%
\chapter{Motivação}\label{motivauxe7uxe3o}}

Dentre o contexto de regressão linear, um conjunto de variáveis pode ser
utilizada para predizer o valor de uma outra variável. O processo de
escolha de um conjunto de variáveis explicativas que melhor predizem a
variável resposta é chamado de modelagem.

A diferença entre o valor predito e o valor verdadeiro da variável de
estudo é chamado de resíduo.

Uma das formas de estimação de um modelo linear é minimizando o o erro
total do modelo, ou seja, encontrando o modelo que minimza o valor do
resíduo.

De maneira detalhada, deseja-se estimar o modelo que minimiza a soma dos
resíduos ao quadrado

\[SRQ = \sum^n_{i=0} (y_i - \hat y_i)^2\]

A SRQ pode ser chamada de uma função de custo.

A minimização entra na área de otimização matematica em otimização.

Existem diversos métodos

\begin{itemize}
\item
  Mínimos Quadrados
\item
  Função de Verossimilhança
\item
  Gradiente Descendente
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{funuxe7uxe3o-de-custo}{%
\chapter{Função de custo}\label{funuxe7uxe3o-de-custo}}

\bookmarksetup{startatroot}

\hypertarget{cuxe1lculos}{%
\chapter{Cálculos}\label{cuxe1lculos}}

O gradiente em relação em relação aos pesos é dado por:

\[D_m = \frac{\delta(Funcao de Custo)}{\delta m } = \frac{\delta}{\delta m}(\frac{1}{n}\sum^n_{i=0}(y_i - \hat y)^2)\]

\[D_m = \frac{2}{n}(\sum (y_i- \hat y_i) \times \frac{\delta}{\delta m}(y_i-\hat y_i))\]

\[D_m = \frac{2}{n}(\sum (y_i- \hat y_i) \times \frac{\delta}{\delta m}(y_i - (mx_i + c)))\]

\[D_m = \frac{2}{n}(\sum (y_i- \hat y_i) \times(-x_i))\]

\[D_m = -\frac{2}{n}(\sum x_i(y_i- \hat y_i))\]

Assim os gradientes sáo dados por

\[D_M = -\frac{1}{n}(\sum x_i (y - \hat y_i))\] e

\[D_C = -\frac{1}{n}(\sum(y_i - \hat y_i))\]

\bookmarksetup{startatroot}

\hypertarget{variauxe7uxf5es}{%
\chapter{Variações}\label{variauxe7uxf5es}}

::: \{.content-visible when-format=``html''\}

\begin{figure}

{\centering 

}

\caption{\label{fig-tesseract}Animation of a tesseract, a cube changing
over time.}

\end{figure}

\bookmarksetup{startatroot}

\hypertarget{muxednimos-quadrados-x-gradiente-descendente}{%
\chapter{Mínimos Quadrados x Gradiente
Descendente}\label{muxednimos-quadrados-x-gradiente-descendente}}

Uma pergunta comum no contexto de regressão linear é: \textbf{Por que
utilizar um método iterativo quando já conhecemos uma fórmula direta
para calcular os coeficientes?}

A resposta principal está na formulação em forma fechada da solução,
dada por:

\[\beta = (X^TX)^{-1}X^TY\]

onde

\begin{itemize}
\item
  \(X\) é a matriz de planejamento, contendo os valores das variáveis
  explicativas (ou preditoras);
\item
  \(Y\) é o vetor coluna que representa os valores da variável resposta
  (ou dependente).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Inversão de Matriz}
\end{enumerate}

A primeira parte da fórmula envolve a inversão da matriz \((X^TX)\).
Embora a inversão de uma matriz seja possível em teoria, em termos
computacionais ela pode ser extremamente custosa. O processo de inversão
tem um custo computacional que aumenta exponencialmente com o tamanho da
matriz, especialmente à medida que o número de variáveis explicativas
(ou colunas de X) aumenta.

\textbf{O custo de inverter uma matriz \(n \times n\) é aproximadamente
\(O(N^3)\) o que significa que, para dados com muitas variáveis, o tempo
de execução cresce de maneira cúbica.}

\textbf{Além disso, se a matriz \(X^TX\) for mal-condicionada (ou seja,
quando algumas variáveis são altamente correlacionadas entre si), o
processo de inversão pode se tornar instável, resultando em erros
numéricos.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Eficiência Computacional e Métodos Iterativos}
\end{enumerate}

Devido ao elevado custo de calcular diretamente a inversa dE \(X^TX\)
métodos iterativos, como os baseados em gradiente, são frequentemente
preferidos em cenários com grandes conjuntos de dados ou alta
dimensionalidade. Esses métodos não exigem a inversão direta da matriz e
podem fornecer aproximações suficientemente boas para os coeficientes
\(\beta\) com um custo computacional muito menor.

\bookmarksetup{startatroot}

\hypertarget{funuxe7uxf5es-de-custo-baseados-na-verossimilhanuxe7a}{%
\chapter{Funções de custo baseados na
Verossimilhança}\label{funuxe7uxf5es-de-custo-baseados-na-verossimilhanuxe7a}}

\bookmarksetup{startatroot}

\hypertarget{gd---caso-loguxedstico}{%
\chapter{GD - Caso Logístico}\label{gd---caso-loguxedstico}}

A regressão logística é utilizada quando o desejamos classficar alguma
classe. É um método pertencente a classse dos MLGs e possui certes
diferenças para a aplaicação do me´todo de Gradiente Descendente

A regressão logística é dada pela seguinte função

\$\$

\bookmarksetup{startatroot}

\hypertarget{summary}{%
\chapter{Summary}\label{summary}}

In summary, this book has no content whatsoever.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\end{CSLReferences}



\end{document}
