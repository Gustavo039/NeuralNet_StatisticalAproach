<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Redes Neurais Artificiais: um bom lugar para um estatístico se deitar - 1&nbsp; Regressão Linear e Gradiente Descendente</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./neuron.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Regressão Linear e Gradiente Descendente</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Redes Neurais Artificiais: um bom lugar para um estatístico se deitar</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Prefácio</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./GD.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Regressão Linear e Gradiente Descendente</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./neuron.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Redes 1: Neuronio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#regressão-linear" id="toc-regressão-linear" class="nav-link active" data-scroll-target="#regressão-linear"><span class="toc-section-number">2</span>  Regressão Linear</a></li>
  <li><a href="#estimação-dos-parâmetros" id="toc-estimação-dos-parâmetros" class="nav-link" data-scroll-target="#estimação-dos-parâmetros"><span class="toc-section-number">3</span>  Estimação dos Parâmetros</a></li>
  <li><a href="#gradiente-descendente" id="toc-gradiente-descendente" class="nav-link" data-scroll-target="#gradiente-descendente"><span class="toc-section-number">4</span>  Gradiente Descendente</a>
  <ul class="collapse">
  <li><a href="#o-gradiente-descendente-como-alternativa-à-ols" id="toc-o-gradiente-descendente-como-alternativa-à-ols" class="nav-link" data-scroll-target="#o-gradiente-descendente-como-alternativa-à-ols"><span class="toc-section-number">4.1</span>  O Gradiente Descendente como Alternativa à OLS</a></li>
  </ul></li>
  <li><a href="#motivação" id="toc-motivação" class="nav-link" data-scroll-target="#motivação"><span class="toc-section-number">5</span>  Motivação</a></li>
  <li><a href="#função-de-custo" id="toc-função-de-custo" class="nav-link" data-scroll-target="#função-de-custo"><span class="toc-section-number">6</span>  Função de custo</a></li>
  <li><a href="#cálculos" id="toc-cálculos" class="nav-link" data-scroll-target="#cálculos"><span class="toc-section-number">7</span>  Cálculos</a></li>
  <li><a href="#variações" id="toc-variações" class="nav-link" data-scroll-target="#variações"><span class="toc-section-number">8</span>  Variações</a></li>
  <li><a href="#gd---caso-logístico" id="toc-gd---caso-logístico" class="nav-link" data-scroll-target="#gd---caso-logístico"><span class="toc-section-number">9</span>  GD - Caso Logístico</a></li>
  <li><a href="#lista-das-principais-funções-de-custo" id="toc-lista-das-principais-funções-de-custo" class="nav-link" data-scroll-target="#lista-das-principais-funções-de-custo"><span class="toc-section-number">10</span>  Lista das Principais Funções de Custo</a>
  <ul class="collapse">
  <li><a href="#função-de-custo-exponencial" id="toc-função-de-custo-exponencial" class="nav-link" data-scroll-target="#função-de-custo-exponencial"><span class="toc-section-number">10.1</span>  Função de Custo Exponencial</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Regressão Linear e Gradiente Descendente</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="regressão-linear" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Regressão Linear</h1>
<p>A regressão linear simples é um método estatístico utilizado para modelar a relação entre duas variáveis: uma variável dependente (ou resposta), que desejamos prever, e uma variável independente (ou explicativa), que usamos para fazer essa previsão. O objetivo da regressão linear é encontrar uma função linear que melhor descreva essa relação.</p>
<p>Por exemplo: podemos utilizar o número de banheiros e quartos de uma casa para predizer seu valor, o número de gols de um atacante utilizando seu histórico dos últimos anos e sua idade ou ainda em um cenário atual, muitas startups que desenvolvem tecnologias de inteligência artificial estão usando regressão linear para prever receitas futuras com base em métricas como usuários ativos, engajamento em plataformas digitais e número de clientes corporativos.</p>
<p>Dentro de uma visão matemática e estatística, um modelo de regressão linear possui uma das estruturas mais simples, dado por:</p>
<p><span class="math display">\[Y = \beta_0 +\beta_1x_1 + \epsilon\]</span></p>
<p>onde</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> é a variável resposta</p></li>
<li><p><span class="math inline">\(\beta_0\)</span> é o intercepto</p></li>
<li><p><span class="math inline">\(\beta_1\)</span> é o coeficiente que pondera a variável explicativa <span class="math inline">\(x_1\)</span></p></li>
<li><p><span class="math inline">\(\epsilon\)</span> é o erro aleatório associado ao modelo. Veremos mais pra frente suas pressuposições e como ele se torna um dos elementos principais para a construção de um modelo lienar</p></li>
</ul>
<p>A essência da regressão linear simples está na tentativa de ajustar uma linha reta aos dados que minimiza a diferença entre os valores observados de <span class="math inline">\(Y\)</span> e os valores preditos pela linha, utilizando para isso as variáveis explicativas</p>
<p>O que torno um modelo em um modelo estatístico são</p>
<p>O cáculo desses coeficientes possui forma fechada, dada por: <span class="math display">\[(X^TX)^{-1}X^TY\]</span></p>
<p>Pela seguinte demonstração</p>
<p>Seja <span class="math inline">\(X\)</span> a matriz de planejamento. Para um conjunto de dados com <span class="math inline">\(p\)</span> variáveis explicativas e <span class="math inline">\(n\)</span> observações, temos um matriz de planejamento de dimensão <span class="math inline">\(n \ \times \ p\)</span></p>
</section>
<section id="estimação-dos-parâmetros" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Estimação dos Parâmetros</h1>
<p>Para se estimar os parametros de uma regressão linear o pensamento mais ismples é: <em>qual o valor dos paramaetros que me retorna o menor erro?</em></p>
<p>Ou seja, para quais valores dos coeficientes <span class="math inline">\(\beta_0,\beta_1, ..., \beta_p\)</span> temos o menor erro.</p>
<p>O “menor erro” pode ser estruturada em forma matematica:</p>
<p><span class="math display">\[Erro = E_i= Y_i-\hat Y_i\]</span> <span class="math display">\[Soma Erro Abs = \sum E_i^2\]</span></p>
<p>Por sua vez, essa função pode ser chamada de função de custo. Dentro no universo de estatística, aprednizado de máquina e redes neurais diversas função de custos são definidas e utilizadas, satisfzanedo um nicho e onjetivo especifio</p>
<p>A soma quadratica dos erros é uma das funções mais simples e mais utilizada no contexto de regressão linear</p>
<p>Para sua minização, temos a seguinte estrutura</p>
<p>A resposta principal está na formulação em forma fechada da solução, dada por:</p>
<p><span class="math display">\[\beta = (X^TX)^{-1}X^TY\]</span></p>
<p>onde</p>
<ul>
<li><p><span class="math inline">\(X\)</span> é a matriz de planejamento, contendo os valores das variáveis explicativas (ou preditoras);</p></li>
<li><p><span class="math inline">\(Y\)</span> é o vetor coluna que representa os valores da variável resposta (ou dependente).</p></li>
</ul>
<ol type="1">
<li><strong>Inversão de Matriz</strong></li>
</ol>
<p>A primeira parte da fórmula envolve a inversão da matriz <span class="math inline">\((X^TX)\)</span>. Embora a inversão de uma matriz seja possível em teoria, em termos computacionais ela pode ser extremamente custosa. O processo de inversão tem um custo computacional que aumenta exponencialmente com o tamanho da matriz, especialmente à medida que o número de variáveis explicativas (ou colunas de X) aumenta.</p>
<p>O custo de inverter uma matriz <span class="math inline">\(n \times n\)</span> é aproximadamente <span class="math inline">\(O(N^3)\)</span> o que significa que, para dados com muitas variáveis, o tempo de execução cresce de maneira cúbica.</p>
<p>Além disso, se a matriz <span class="math inline">\(X^TX\)</span> for mal-condicionada (ou seja, quando algumas variáveis são altamente correlacionadas entre si), o processo de inversão pode se tornar instável, resultando em erros numéricos.</p>
<ol start="2" type="1">
<li><strong>Eficiência Computacional e Métodos Iterativos</strong></li>
</ol>
<p>Devido ao elevado custo de calcular diretamente a inversa dE <span class="math inline">\(X^TX\)</span> métodos iterativos, como os baseados em gradiente, são frequentemente preferidos em cenários com grandes conjuntos de dados ou alta dimensionalidade. Esses métodos não exigem a inversão direta da matriz e podem fornecer aproximações suficientemente boas para os coeficientes <span class="math inline">\(\beta\)</span> com um custo computacional muito menor.</p>
</section>
<section id="gradiente-descendente" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Gradiente Descendente</h1>
<p>O gradiente é um conceito central em otimização. Ele representa a direção e a intensidade de variação de uma função com respeito a cada um de seus parâmetros. Em termos mais intuitivos, o gradiente aponta “em que direção” e “o quão rapidamente” o valor da função de custo aumenta ou diminui em relação aos parâmetros.</p>
<p>Para uma função de custo <span class="math inline">\(J(\theta)\)</span> onde <span class="math inline">\(\theta\)</span> representa o vetor de parâmetros, o gradinete de <span class="math inline">\(\nabla J(\theta)\)</span> é o vetor de derivadas parciais:</p>
<p><span class="math inline">\(\nabla J(\theta) = (\frac{\partial J}{\partial\theta_1},\frac{\partial J}{\partial\theta_2},...,\frac{\partial J}{\partial\theta_n})\)</span></p>
<p>Cada componente desse vetor indica como uma pequena alteração em um parâmetro específico impacta <span class="math inline">\(\theta_i\)</span> o valor da função de custo. No contexto de minimização, o gradiente fornece a direção em que a função de custo aumenta mais rapidamente. Portanto, para minimizar <span class="math inline">\(J(\theta)\)</span> ajustamos os parâmetros no sentido oposto ao gradiente — daí o nome “gradiente descendente”.</p>
<section id="o-gradiente-descendente-como-alternativa-à-ols" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="o-gradiente-descendente-como-alternativa-à-ols"><span class="header-section-number">4.1</span> O Gradiente Descendente como Alternativa à OLS</h2>
<p>O gradiente descendente é uma técnica iterativa de otimização que ajusta os parâmetros na direção oposta ao gradiente, com o objetivo de encontrar o ponto de mínimo da função de custo. A atualização dos parâmetros em cada iteração é feita com a seguinte fórmula:</p>
<p><span class="math display">\[\theta^{(t+1)} = \theta(t) -\eta\nabla J(\theta^{(t)}) \]</span> Onde:</p>
<ul>
<li><p><span class="math inline">\(\theta^{(t+1)}\)</span> é o vetor de parâmetros atualizado.</p></li>
<li><p><span class="math inline">\(\theta^{(t)}\)</span> é o vetor de parâmetros na iteração atual.</p></li>
<li><p><span class="math inline">\(\nabla\)</span> é a taxa de aprendizado (learning rate), um hiperparâmetro que controla o tamanho dos passos de atualização.</p></li>
<li><p><span class="math inline">\(\nabla J(\theta(t))\)</span> é o gradiente da função de custo calculado com base nos parâmetros da iteração atual.</p></li>
</ul>
</section>
</section>
<section id="motivação" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Motivação</h1>
<p>Dentre o contexto de regressão linear, um conjunto de variáveis pode ser utilizada para predizer o valor de uma outra variável. O processo de escolha de um conjunto de variáveis explicativas que melhor predizem a variável resposta é chamado de modelagem.</p>
<p>A diferença entre o valor predito e o valor verdadeiro da variável de estudo é chamado de resíduo.</p>
<p>Uma das formas de estimação de um modelo linear é minimizando o o erro total do modelo, ou seja, encontrando o modelo que minimza o valor do resíduo.</p>
<p>De maneira detalhada, deseja-se estimar o modelo que minimiza a soma dos resíduos ao quadrado</p>
<p><span class="math display">\[SRQ = \sum^n_{i=0} (y_i - \hat y_i)^2\]</span></p>
<p>A SRQ pode ser chamada de uma função de custo.</p>
<p>A minimização entra na área de otimização matematica em otimização.</p>
<p>Existem diferentes métodos</p>
<ul>
<li><p>Métodos Mínimos Quadrados</p></li>
<li><p>Método via Verossimilhança</p></li>
<li><p>Gradiente Descendente</p></li>
</ul>
</section>
<section id="função-de-custo" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Função de custo</h1>
</section>
<section id="cálculos" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Cálculos</h1>
<p>O gradiente em relação em relação aos pesos é dado por:</p>
<p><span class="math display">\[D_m = \frac{\partial(Funcao de Custo)}{\partial m } = \frac{\partial}{\partial m}(\frac{1}{n}\sum^n_{i=0}(y_i - \hat y)^2)\]</span></p>
<p><span class="math display">\[D_m = \frac{2}{n}(\sum (y_i- \hat y_i) \times \frac{\partial}{\partial m}(y_i-\hat y_i))\]</span></p>
<p><span class="math display">\[D_m = \frac{2}{n}(\sum (y_i- \hat y_i) \times \frac{\partial}{\partial m}(y_i - (mx_i + c)))\]</span></p>
<p><span class="math display">\[D_m = \frac{2}{n}(\sum (y_i- \hat y_i) \times(-x_i))\]</span></p>
<p><span class="math display">\[D_m = -\frac{2}{n}(\sum x_i(y_i- \hat y_i))\]</span></p>
<p>Assim os gradientes sáo dados por</p>
<p><span class="math display">\[D_M = -\frac{1}{n}(\sum x_i (y - \hat y_i))\]</span> e</p>
<p><span class="math display">\[D_C = -\frac{1}{n}(\sum(y_i - \hat y_i))\]</span></p>
</section>
<section id="variações" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Variações</h1>
<p>::: {.content-visible when-format=“html”}</p>
<div id="fig-tesseract" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div align="center">
<iframe width="700" height="700" src="./images/gif_GD_tipos.gif"></iframe>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;8.1: Animation of a tesseract, a cube changing over time.</figcaption><p></p>
</figure>
</div>
</section>
<section id="gd---caso-logístico" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> GD - Caso Logístico</h1>
<p>A regressão logística é utilizada quando o desejamos classficar alguma classe. É um método pertencente a classse dos MLGs e possui certes diferenças para a aplaicação do me´todo de Gradiente Descendente</p>
<p>A regressão logística é dada pela seguinte função</p>
<p>$$</p>
<p>$$</p>
<p>A principal difernça está na definição da função de custo. Enquanto que na regressão linear a principais funções de custo são a soma dos resíduos ao quadrado ou o erro quadratico médio, para o caso logistico utiliza-se o logaritimo da função que representa a regressão logistica, chamada de Binary Cross-Entropy Loss (Log Loss).</p>
<p>A Binary Cross-Entropy Loss se deriva do custo de erro, dado por:</p>
<p><span class="math display">\[\text{custo}(h_\theta(x),y)  = \begin{cases}
      -\log(h_{\theta}(x)) , &amp; \text{if } y = 1 \\
      -\log(1 - h_{\theta}(x)) , &amp; \text{if } y = 0
   \end{cases}\]</span></p>
<p>Ou de maneira unificada</p>
<p><span class="math display">\[\text{custo}(h_{\theta}(x), y) = -y^{(i)} \times \log(h_{\theta}(x^{(i)})) - (1 - y^{(i)}) \times \log(h_{\theta}(x^{(i)}))\]</span></p>
<p>Para <span class="math inline">\(m\)</span> observações, a métrica pode ser simplificada como a média:</p>
<p><span class="math display">\[J(\theta) = -\frac{1}{m}\sum_{i=1}^m y^{(i)} \times \log(h_{\theta}(x^{(i)})) - (1 - y^{(i)}) \times \log(h_{\theta}(x^{(i)}))\]</span></p>
<p>Assim como no caso da regressão linear, o objetivo é minimizar a função <span class="math inline">\(J(\theta)\)</span></p>
<p>Dado um total de <span class="math inline">\(n\)</span> variáveis, assumimos um total de <span class="math inline">\(n\)</span> parâmetros para o vetor <span class="math inline">\(\theta\)</span>. Para minimzar <span class="math inline">\(J(\theta)\)</span>, temos que realizar um Gradiente Descendente em cada parametro de <span class="math inline">\(\theta\)</span>, denominado <span class="math inline">\(\theta_j\)</span>. Onde a cada iteração, a seguinte atualização é realizada</p>
<p><span class="math display">\[\theta_j \leftarrow \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \]</span> Na etapa final do algoritmo, precisamos rodar a o gradiente descendente simultaneamente em cada parametro, ou seja, atualizar <span class="math inline">\(\theta_0, \theta_1, ..., \theta_n\)</span> contidos no vetor <span class="math inline">\(\mathbf{\theta}\)</span></p>
<p>A atualização em <span class="math inline">\(\theta_j\)</span> é computada a partir de sua derivada</p>
<p><span class="math display">\[\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j\]</span></p>
<p>E portanto, substituindo na formula da atualização, temos a seguinte regra:</p>
<p><span class="math display">\[\theta_j \leftarrow \theta_j - \alpha \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j\]</span></p>
</section>
<section id="lista-das-principais-funções-de-custo" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Lista das Principais Funções de Custo</h1>
<p>Alem das funções de custo ja listadas, outras são definidas para determinados nichos e obejtivos. O seguinte tópico buscou listar as principais.</p>
<section id="função-de-custo-exponencial" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="função-de-custo-exponencial"><span class="header-section-number">10.1</span> Função de Custo Exponencial</h2>
<p>Utilizada para o algoritmo AdaBoost de classsificação, onde sua forma de convexidade e crescimento exponencial para valroes negativos a torna sensivel para valores outliers. Dada por</p>
<p><span class="math display">\[f(x) = \frac{1}{2}log(\frac{p(1|x)}{1-p(1|x)})\]</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Prefácio</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./neuron.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Redes 1: Neuronio</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>