<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Redes Neurais Artificiais: um bom lugar para um estatístico se deitar - 2&nbsp; Neuronios e Camadas</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./summary.html" rel="next">
<link href="./GD.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Neuronios e Camadas</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Redes Neurais Artificiais: um bom lugar para um estatístico se deitar</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Prefácio</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./GD.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Regressão Linear e Gradiente Descendente</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./neuron.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Neuronios e Camadas</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#estrutura-básica" id="toc-estrutura-básica" class="nav-link active" data-scroll-target="#estrutura-básica"><span class="toc-section-number">3</span>  Estrutura Básica</a></li>
  <li><a href="#camadas" id="toc-camadas" class="nav-link" data-scroll-target="#camadas"><span class="toc-section-number">4</span>  Camadas</a></li>
  <li><a href="#função-de-ativação" id="toc-função-de-ativação" class="nav-link" data-scroll-target="#função-de-ativação"><span class="toc-section-number">5</span>  Função de Ativação</a></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation"><span class="toc-section-number">6</span>  Backpropagation</a>
  <ul class="collapse">
  <li><a href="#pesos-e-vieses" id="toc-pesos-e-vieses" class="nav-link" data-scroll-target="#pesos-e-vieses"><span class="toc-section-number">6.1</span>  Pesos e Vieses</a></li>
  <li><a href="#equações" id="toc-equações" class="nav-link" data-scroll-target="#equações"><span class="toc-section-number">6.2</span>  Equações</a>
  <ul class="collapse">
  <li><a href="#equação-para-o-erro-na-camada-de-saída" id="toc-equação-para-o-erro-na-camada-de-saída" class="nav-link" data-scroll-target="#equação-para-o-erro-na-camada-de-saída"><span class="toc-section-number">6.2.1</span>  Equação para o erro na camada de saída</a></li>
  <li><a href="#equação-para-o-erro" id="toc-equação-para-o-erro" class="nav-link" data-scroll-target="#equação-para-o-erro"><span class="toc-section-number">6.2.2</span>  Equação para o erro</a></li>
  <li><a href="#equação-para-a-taxa-de-variação-do-custo-em-relação-aos-vieses" id="toc-equação-para-a-taxa-de-variação-do-custo-em-relação-aos-vieses" class="nav-link" data-scroll-target="#equação-para-a-taxa-de-variação-do-custo-em-relação-aos-vieses"><span class="toc-section-number">6.2.3</span>  Equação para a taxa de variação do custo em relação aos vieses</a></li>
  <li><a href="#equação-para-a-taxa-de-variação-do-custo-em-relação-aos-pesos" id="toc-equação-para-a-taxa-de-variação-do-custo-em-relação-aos-pesos" class="nav-link" data-scroll-target="#equação-para-a-taxa-de-variação-do-custo-em-relação-aos-pesos"><span class="toc-section-number">6.2.4</span>  Equação para a taxa de variação do custo em relação aos pesos</a></li>
  </ul></li>
  <li><a href="#algoritmo" id="toc-algoritmo" class="nav-link" data-scroll-target="#algoritmo"><span class="toc-section-number">6.3</span>  Algoritmo</a></li>
  </ul></li>
  <li><a href="#gd-e-bck-em-conjunto" id="toc-gd-e-bck-em-conjunto" class="nav-link" data-scroll-target="#gd-e-bck-em-conjunto"><span class="toc-section-number">7</span>  GD e BCK em Conjunto</a>
  <ul class="collapse">
  <li><a href="#exemplo" id="toc-exemplo" class="nav-link" data-scroll-target="#exemplo"><span class="toc-section-number">7.1</span>  Exemplo</a></li>
  <li><a href="#definindo-os-valores" id="toc-definindo-os-valores" class="nav-link" data-scroll-target="#definindo-os-valores"><span class="toc-section-number">7.2</span>  Definindo os Valores</a>
  <ul class="collapse">
  <li><a href="#forward-pass" id="toc-forward-pass" class="nav-link" data-scroll-target="#forward-pass"><span class="toc-section-number">7.2.1</span>  Forward Pass</a></li>
  <li><a href="#calculando-o-erro-total" id="toc-calculando-o-erro-total" class="nav-link" data-scroll-target="#calculando-o-erro-total"><span class="toc-section-number">7.2.2</span>  Calculando o Erro Total</a></li>
  <li><a href="#backward-pass" id="toc-backward-pass" class="nav-link" data-scroll-target="#backward-pass"><span class="toc-section-number">7.2.3</span>  Backward Pass</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Neuronios e Camadas</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Dado as definições iniciais das regressões linear e logistica, assim como do metodo de gradiente descendente, podemos passar para a definiçãop de uma Rede Neural. Para um bom entendimento de como funciona uma rede, destricharemos em 3 etapas:</p>
<ul>
<li>Arquitetura</li>
<li>Tipos</li>
<li>Aplicação</li>
</ul>
<p>Um neurônio em uma rede neural artificial é uma unidade computacional inspirada no funcionamento de um neurônio biológico. Ele recebe múltiplas entradas (inputs), realiza uma operação matemática para processar esses valores e gera uma saída (output). Essa operação geralmente envolve uma soma ponderada das entradas seguida por uma função de ativação, que transforma o valor resultante antes de enviá-lo à próxima camada da rede.</p>
<p>O neurônio humano é uma célula especializada no sistema nervoso, composta por dendritos, corpo celular (soma), axônio e terminais do axônio (sinapses). Os dendritos recebem sinais químicos e elétricos de outros neurônios, que se acumulam no corpo celular, onde esses sinais são integrados. Se a soma desses sinais ultrapassar um certo limiar, o neurônio gera um impulso elétrico, conhecido como potencial de ação. Esse impulso viaja pelo axônio até os terminais, onde é convertido novamente em sinal químico. Nessa etapa, neurotransmissores são liberados para se comunicar com outros neurônios através das sinapses, formando uma complexa rede de comunicação e processamento. Cada neurônio humano possui milhares de conexões (sinapses) com outros neurônios, o que permite um processamento de informações altamente paralelo e dinâmico, com adaptações complexas que envolvem neuroplasticidade ao longo do tempo.</p>
<p>Já um neurônio computacional, usado em redes neurais artificiais, é uma representação simplificada do neurônio humano. Ele possui uma estrutura mais básica, composta por entradas, uma soma ponderada e uma função de ativação. Cada entrada do neurônio computacional tem um peso associado, que representa a importância dessa entrada para o resultado final. O neurônio computacional calcula a soma ponderada das entradas, aplica uma função de ativação para transformar o resultado e então gera uma saída. Diferente do neurônio biológico, que pode transmitir informações de maneira complexa e em várias direções, o neurônio computacional apenas encaminha sua saída para os neurônios da próxima camada da rede.</p>
<p>Basicamento, podemos representar uma neuronio humano e computacional da seguinte forma</p>
<p><img src="images/neuron.png" class="img-fluid"></p>
<p>Um conjunto de neurônios humanos interconectados é conhecido como rede de neurônios ou <strong>rede neural</strong>. No cérebro humano, essas redes formam complexas interconexões chamadas de circuitos neurais, que são responsáveis pelo processamento de informações e pela comunicação entre diferentes partes do sistema nervoso.</p>
<p>No contexto computacional um conjunto de neurônios também é chamado de uma rede neural.</p>
<section id="estrutura-básica" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Estrutura Básica</h1>
<p>Um neurónio é a estrura mais básica de uma rede neural. Ele recebe um valor, processa ele, e retorna outro valor que é passado para o neuronio seguinte.</p>
<p>Em uma linguagem matemática, seja um neurônio <span class="math inline">\(K\)</span> <span class="math inline">\(x_1,x_2, ....,x_m\)</span> variáveis, <span class="math inline">\(m+1\)</span> entradas (<em>inputs</em>) e um vetor de pesos <span class="math inline">\(w_1,w_2, ..., w_m\)</span>.</p>
<ol type="1">
<li>Bias Input</li>
</ol>
<ul>
<li>O input <span class="math inline">\(x_0\)</span> é definido como o valor constante <span class="math inline">\(+1\)</span>. Isso o torna o chamado <strong>Bias Input</strong>, que é utilizado para ajustar o valor de saida do neurônio independente dos damais valores de entrada. Esse termo permite que o neurônio retorne valores diferentes de 0 mesmo quando todos os valores de entrada são iguais a 0.</li>
</ul>
<ol start="2" type="1">
<li>Actual Inputs</li>
</ol>
<ul>
<li>Os valores de entradas remanescente, vindo das variáveis <span class="math inline">\(x_1,x_2, ...x_m\)</span> são chamadas de entradas verdadeiras (<strong>Actual Inputs</strong>), tendo seu próprio vetor de peso associado <span class="math inline">\(w_{k1}, w_{k2},..., w_{km}\)</span></li>
</ul>
<ol start="3" type="1">
<li>Soma Ponderada</li>
</ol>
<ul>
<li>A soma dos valores de entrada ponderadas pelos pesos é realizada, dada por:</li>
</ul>
<p><span class="math display">\[z_k = \phi(\sum^m_{j=0}w_{kj}x_j) + b\]</span></p>
</section>
<section id="camadas" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Camadas</h1>
<p>Em rede neural, o fluxo dos dados percorre os neuronios atraves de determinados caminhos, chamados de <strong>Camadas</strong>.</p>
<p>Uma camada (layer) pode ser caracterizada como um conjunto de neuronios, onde neuronios de diferentes camadas possuem comunicação, porem neuronios de uma mesma camadas não possuem nenhuma ligação</p>
<p><img src="images/simple_nnet.png" class="img-fluid"></p>
<p>Uma rede possui 3 camadas distintas: Entrada (Input), Oculta (Hidden) e Saida (Output)</p>
<ul>
<li>Camada de Entrada
<ul>
<li>Se caracteriza como a primeira camada de uma rede e é responsável em receber os dados do conjunto de dados utilizado. No caso de um conjunto de dados estruturado, possuindo uma organização padrão de variáveis em colunas e observações em linhas, cada variável seria atribuido a um neuronio distinto, ou seja, se um conjunto de dados possui 10 diferentes variaveis, serão necessários 10 diferentes neuronios na camada de entrada. No caso de conjunto de dados não estruturados como imagens e sons, cada neuronio pode receber um pixel ou um determinado intervalo do som. Vale destacar que uma rede sempre vai possuir uma e somente uma camada de entrada</li>
</ul></li>
<li>Camadas Ocultas
<ul>
<li>Se caracteriza como as camadas que estão entre a camada de entrada e a camadas de saída, e não possui um número fixo. Redes mais simples possuem entre 1 a 3 camadas ocultas. Para problemas mais complexos esse número aumenta. A definição do número de camadas ocultas pode se basear em 2 abordagems: Fixar ou Otimizar.</li>
<li>Fixar o número de camadas ocultas significa definir um valor fixo antes de iniciar o treinamento do modelo. Essa abordagem é baseada em escolhas prévias, geralmente fundamentadas no conhecimento do problema ou em heurísticas. Redes simples, que resolvem problemas com poucos dados ou baixa complexidade, muitas vezes utilizam entre 1 e 3 camadas ocultas. Esse método é vantajoso em cenários onde se busca simplicidade no design do modelo, menor custo computacional e rapidez no desenvolvimento.</li>
<li>Otimizar o número de camadas ocultas é uma abordagem que leva a observamos esse número como hiperparâmetro, e assim aplicar tecnicas de tuning para encontrar um número ótimo. Essa opção é utilizada quando não temos nenhum conhecimento a priori do preblema e redes que ja foram utilizadas em problemas semelhantes.</li>
</ul></li>
<li>Camada de Saida
<ul>
<li>É a última camada de uma rede e é responsável em passar os valores finais preditos. Toda rede vai possuir uma e somente uma camada de saída, onde o número de neurônios dessa camada vai depender do contexto do problema. Em um problema de classificação, devemos ter um neuronio para cada classe. Já ára um problema de regressão, devemos definir um neurinio para cada dimensão, ou seja para uma regressão com a predição de apenas um valor (univariada) devemos ter apenas 1 neurônio, ja para regressão com <span class="math inline">\(m\)</span> valores preditos (multivariada), devemos definir <span class="math inline">\(m\)</span> neurônios nessa camada</li>
</ul></li>
</ul>
</section>
<section id="função-de-ativação" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Função de Ativação</h1>
<p>Como definimos na seção “Estrutura Básica”, um neurônio é definido algebricamente como <span class="math display">\[z_k = \phi(\sum^m_{j=0}w_{kj}x_j) + b\]</span>, onde <span class="math inline">\(phi\)</span> é uma função chamada de função de ativação. Ela é responsável por transformar o valor bruto calculado por um neurônio (o somatório ponderado das entradas mais o viés) em uma saída que será transmitida para os neurônios da próxima camada. O principal objetivo dessa transformação é introduzir não-linearidade no modelo, permitindo que ele aprenda padrões complexos nos dados.</p>
<p>Sem uma função de ativação, a rede neural seria equivalente a uma combinação linear das entradas, independentemente de quantas camadas fossem adicionadas. Isso significa que ela só poderia resolver problemas simples e lineares. A introdução de não-linearidade torna possível a resolução de problemas mais desafiadores, como a classificação de dados que não podem ser separados por uma linha ou plano.</p>
<p>Atualmente, existem diversas funções de ativação já definidas, onde cada uma possui uma nicho específico de utilização. Listamos as mais conhecidas</p>
<ul>
<li><p>Sigmoide</p>
<ul>
<li><span class="math inline">\(f(x) = \frac{1}{1+e^{-x}}\)</span></li>
<li>Transforma a saída para o entre intervalo 0 e 1, sendo ideal para tarefas de classificação binária.</li>
<li>Extremamente popular em redes mais antigas, porém tem limitações como a saturação e o problema do “vanishing gradient”, fenômeno esse que se caracteriza quando a função de ativação gera gradientes tão pequenos, que diminuem ainda mais à medida que são propagados para camadas anteriores -&gt; Colocar foto da função, sua equação</li>
</ul></li>
<li><p>Tanh (Tangente Hiperbólica)</p>
<ul>
<li><span class="math inline">\(f(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}\)</span></li>
<li>Transforma a saída para o entre intervalo -1 e 1 -&gt; Colocar foto da função, sua equação</li>
</ul></li>
<li><p>ReLU (Rectified Linear Unit)</p>
<ul>
<li><span class="math inline">\(f(x) = \max(0,x)\)</span></li>
</ul></li>
<li><p>Leaky ReLu (ReLU Contaminada)</p>
<ul>
<li><p><span class="math inline">\(f(x) = \begin{cases} x, &amp; \text{se } x&gt;0 \\ \alpha x, &amp; \text{se } x\leq 0 \end{cases}\)</span></p></li>
<li><p><span class="math inline">\(\alpha\)</span> é uma constante que irar ponderar os valores de x menores ou iguais a 0, podendo ser fixada antes do treinamento do modelo, ou classificada como um parâmetro, passando pela etapa de otimização</p></li>
</ul></li>
<li><p>Softmax</p>
<ul>
<li><span class="math inline">\(\sigma(z_i) = \frac{e^{z_i}}{\sum^L_{j=i}e^{zj}}\)</span></li>
</ul></li>
</ul>
<p>Vale destacar que <strong>uma função de ativação e função de custo são coisas DISTINTAS</strong>. Enquanto uma função de ativação é aplicada localmente em cada neurônio, uma função de custo é aplicada apenas na sáida final da rede, ou seja, no momento da predição de valores, tendo o objetivo de calcular o erro global da rede. Essa função é utilizada para a otimização dos pesos e vieses durante o treinamento. Resumidamente, a função de ativação transforma os sinais dentro da rede para capturar padrões, enquanto a função de custo avalia o erro final das previsões para guiar o aprendizado do modelo.</p>
</section>
<section id="backpropagation" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Backpropagation</h1>
<p>O objetivo do Backpropagation é calcular as derivadas parciais <span class="math inline">\(\frac{\partial C}{\partial w}\)</span> e <span class="math inline">\(\frac{\partial C}{\partial b}\)</span>, onde <span class="math inline">\(C\)</span> é função de custo, <span class="math inline">\(w\)</span> é o peso (weight) e <span class="math inline">\(b\)</span> é o viés (bias). Para o método funcionar, precisamos definir duas suposições</p>
<ol type="1">
<li>Generalização pela media</li>
</ol>
<ul>
<li>A função de custo <span class="math inline">\(C\)</span> pode ser reescrita como <span class="math inline">\(C = \frac{1}{n}\sum_xC_x\)</span>. Isso deve ser assumido por conta da forma que o método calcula as derivadas parciais <span class="math inline">\(\frac{\partial C}{\partial w}\)</span> e <span class="math inline">\(\frac{\partial C}{\partial b}\)</span>, onde dado <span class="math inline">\(x\)</span> o conjunto de dados de treinamento da iteração, temos na verdade as derivadas <span class="math inline">\(\frac{\partial C_x}{\partial w}\)</span> e <span class="math inline">\(\frac{\partial C_x}{\partial b}\)</span>.</li>
</ul>
<ol start="2" type="1">
<li>Função de custo pode ser reescrita como uma função da saída (output) da rede</li>
</ol>
<ul>
<li>Dado os neuronios da camada de saida <span class="math inline">\(k^L_1, k^L_2, ..., k^L_j\)</span>, e seus outputs <span class="math inline">\(a^L_1, a^L_2, ..., a^L_j\)</span>, a função de custo <span class="math inline">\(C\)</span> pode ser reescrita como <span class="math inline">\(C = C(a^l)\)</span></li>
</ul>
<section id="pesos-e-vieses" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="pesos-e-vieses"><span class="header-section-number">6.1</span> Pesos e Vieses</h2>
<p>Antes de entrar de fato nas equações e formas do método, devemos fazer definições importantes sobre a nomenclatura de certos parâmetros e suas caracteristicas</p>
<p>Quando falamos de pesos e vieses, há muita confusão sobre a função e aplicação de cada um, onde muitas pessoas utilizam esses termos como sinônimos, o que fortemente não é verdade.</p>
<p>A confusão entre pesos e vieses surge frequentemente porque ambos são parâmetros de uma rede neural que são ajustados durante o treinamento e determinam coletivamente o comportamento do modelo. No entanto, eles servem a propósitos e funções distintas.</p>
<p>Os pesos (weights) são fatores de <strong>multiplicação</strong> aplicados aos valores em cada neuronio e determinam a força e direção da relação entre os valores de entrada e de saída de um neurônio.</p>
<p>Já os vieses (bias) são fatores de <strong>soma</strong> e permitem o deslocamento da função de ativação, auxiliando que o modelo se ajuste melhor aos dados</p>
<p>Dado uma rede com uma função de ativação <span class="math inline">\(At\)</span>, temos o seguinte valor de saida de um determinado neurônio</p>
<p><span class="math display">\[z = At(w_i* x_i) + b\]</span> Vemos que o peso <span class="math inline">\(w_i\)</span> multiplica o valor de entrada <span class="math inline">\(x_i\)</span>, controlando a influência de <span class="math inline">\(x_i\)</span> no neuronio atual. Já o vies <span class="math inline">\(b\)</span> é somado ao valor retornado pela função de ativação, auxiliando o neuronio a melhor se ajustar aos dados</p>
<p>As principais diferenças podem resumidas em uma tabela</p>
</section>
<section id="equações" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="equações"><span class="header-section-number">6.2</span> Equações</h2>
<p>O método se baseia em 4 equações fundamentais</p>
<section id="equação-para-o-erro-na-camada-de-saída" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="equação-para-o-erro-na-camada-de-saída"><span class="header-section-number">6.2.1</span> Equação para o erro na camada de saída</h3>
<p><span class="math display">\[\delta^L_j = \frac{\partial C}{\partial a^L_j}\sigma´(z^L_j)\]</span></p>
<p>O primeiro termo a direta, <span class="math inline">\(\frac{\partial C}{\partial a^L_j}\)</span> mensura o quão rápido a função de custo está se adaptando em relação ao j-ésimo neurônio de saída. Por exemplo, se a função custo não depender muito de um neuronio j em particular, portanto <span class="math inline">\(\delta^L_j\)</span> será um valor pequeno</p>
<p>Já o segundo termo a direita, <span class="math inline">\(\sigma´(z^L_j)\)</span>, mensura o quão rápido a função de ativação <span class="math inline">\(\sigma\)</span> esta mudando em relação a <span class="math inline">\(z^L_j\)</span></p>
<p>Na forma matricial a BP1 possui a seguinte forma</p>
<p><span class="math display">\[\sigma^L = \Delta_aC \odot \sigma´ (z^L)\]</span> Onde, <span class="math inline">\(\Delta_aC\)</span> é definido como o vetor que sias componentes são as derivadas parciais <span class="math inline">\(\frac{\partial C}{\partial a^L_j}\)</span>, para facilitar o entedimento, podemos expressar <span class="math inline">\(\Delta_aC\)</span> como a taxa de variação de <span class="math inline">\(C\)</span> em relação ao ativações de saída</p>
</section>
<section id="equação-para-o-erro" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="equação-para-o-erro"><span class="header-section-number">6.2.2</span> Equação para o erro</h3>
<p>A equação para o erro <span class="math inline">\(\delta^l\)</span> em relação ao erro uam camada a frente, <span class="math inline">\(\delta^{l+1}\)</span> é dado por</p>
<p><span class="math display">\[\delta^l = ((w^{l+1})^T\delta^{l+1})\odot\sigma´(z^l)\]</span></p>
<p>Onde <span class="math inline">\((w^{l+1})^T\)</span> é a matriz transposta da matriz de pesos <span class="math inline">\(w^{l+1}\)</span> para a <span class="math inline">\(l+1\)</span>-ésima camada</p>
</section>
<section id="equação-para-a-taxa-de-variação-do-custo-em-relação-aos-vieses" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="equação-para-a-taxa-de-variação-do-custo-em-relação-aos-vieses"><span class="header-section-number">6.2.3</span> Equação para a taxa de variação do custo em relação aos vieses</h3>
<p><span id="eq-bp1"><span class="math display">\[
\frac{\partial C}{\partial b^l_j} = \delta^l_j
\tag{6.1}\]</span></span></p>
<p>Temos que o erro <span class="math inline">\(\delta^l_j\)</span> é exatamente igual a taxa de variação <span class="math inline">\(\frac{\partial C}{\partial b^l_j}\)</span>. Isso se mostra como um ponto positivo, dado que já sabemos como calcular <span class="math inline">\(\delta^l_j\)</span> como visto nas equações BP1 e BP2. Assim podemos escrever a BP3 como</p>
<p><span class="math display">\[\frac{\partial C}{\partial b} = \delta\]</span></p>
<p>Onde <span class="math inline">\(\delta\)</span> está sendo calculado no mesmo neuronio do viés <span class="math inline">\(b\)</span></p>
</section>
<section id="equação-para-a-taxa-de-variação-do-custo-em-relação-aos-pesos" class="level3" data-number="6.2.4">
<h3 data-number="6.2.4" class="anchored" data-anchor-id="equação-para-a-taxa-de-variação-do-custo-em-relação-aos-pesos"><span class="header-section-number">6.2.4</span> Equação para a taxa de variação do custo em relação aos pesos</h3>
<p><span class="math display">\[\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k\delta^l_j\]</span></p>
</section>
</section>
<section id="algoritmo" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="algoritmo"><span class="header-section-number">6.3</span> Algoritmo</h2>
<p>As funções definidas na seção passada provem uma forma de se calcular o gradiente da função de custo. Essas funções são utilizadas no algortimo do método que possui 5 passos fundamentais</p>
<ol type="1">
<li>Input x</li>
</ol>
<ul>
<li>O conjunto dados é introduzido a rede. No pensamento algebrico, o vetor <span class="math inline">\(X\)</span> é introduzido a rede e atribuido como a ativação da camada de entrada</li>
</ul>
<ol start="2" type="1">
<li>Feedforward</li>
</ol>
<ul>
<li>Para cada camada <span class="math inline">\(l = 2,3, ..., L\)</span> calcula-se os valores retornado pelos neurônios <span class="math inline">\(z^l = w^la^{l-1}+b^l\)</span>, onde <span class="math inline">\(w^l\)</span> é a matriz de peso para a camada <span class="math inline">\(l\)</span>, <span class="math inline">\(a^{l-1}\)</span> é o valor de ativação da camada anterior, <span class="math inline">\(b^l\)</span> é o vetor de vieses para a camada <span class="math inline">\(l\)</span></li>
<li>Além disso, devemos reforçar que <span class="math inline">\(a^l = \sigma(z^l)\)</span>, onde <span class="math inline">\(\sigma\)</span> é uma função de ativação</li>
</ul>
<ol start="3" type="1">
<li>Erro de Saída</li>
</ol>
<ul>
<li>O erro na camada de saída é calculado, dado por <span class="math display">\[\delta^L=\Delta_aC\odot\sigma´(z^L)\]</span></li>
<li><span class="math inline">\(\Delta_aC\)</span> é o gradiente da função de custo C em relação ao valor de ativação <span class="math inline">\(a^L\)</span></li>
<li><span class="math inline">\(\sigma´(z^L)\)</span> é a derivada da função de ligação no valor de ativação <span class="math inline">\(z^l\)</span></li>
<li>Isso quantifica o quanto a saída da rede (ativações) se desvia da saída desejada.</li>
</ul>
<ol start="4" type="1">
<li>Propagação do Erro</li>
</ol>
<ul>
<li>Para cada camada <span class="math inline">\(l = L-1,L-2, ...,2\)</span>, o erro para a camada é calculado, dado por <span class="math display">\[\delta^l = ((w^{l+1})^T\delta^{l+1})\odot\sigma´(z^l)\]</span></li>
<li><span class="math inline">\((w^{l+1})^T\delta^{l+1}\)</span> propaga o erro para trás na rede</li>
<li><span class="math inline">\(\sigma´(z^l)\)</span> ajusta o erro baseado na função de ativação</li>
<li>Nessa etapa o erro é propagado para trás, dando origem ao nome do método <strong>Backpropagation</strong></li>
</ul>
<ol start="5" type="1">
<li>Calculo dos Gradientes</li>
</ol>
<ul>
<li>No último passo do algoritmo, os gradientes em relação aos pesos e vieses são calculados</li>
<li>Para o peso (weight) temos <span class="math display">\[\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k\delta^l_j\]</span></li>
<li>Já para os vieses (bias) <span class="math display">\[\frac{\partial C}{\partial b^l_{j}} = \delta^l_j\]</span></li>
<li>Os valores calculados para cada gradiente são utilizados no momento da otimização via gradiente descendente, minimizando a função de custo</li>
</ul>
</section>
</section>
<section id="gd-e-bck-em-conjunto" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> GD e BCK em Conjunto</h1>
<p>O GD e BCK fazem um trabalho de turma</p>
<p>Relembrando, Gradient Descent é um algoritmo usado para minimizar uma função, neste caso, a função de custo <span class="math inline">\(C(\theta)\)</span>, onde <span class="math inline">\(\theta\)</span> representa a matriz de parâmetros: pesos e vieses</p>
<ul>
<li>O algoritmo atualiza os parâmetros iterativamente na direção do gradiente negativo da perda em relação aos parâmetros: <span class="math display">\[\theta^{(t+1)} = \theta(t) -\eta\nabla J(\theta^{(t)})\]</span></li>
</ul>
<p>O desafio mora no calculo do gradiente <span class="math inline">\(\nabla_{\theta}L(\theta)\)</span>, que é aí que se utiliza o Backpropagation</p>
<p>Backpropagation é um algoritmo para calcular os gradientes da função de custo em relação aos pesos e vieses da rede de forma eficiente usando a regra da cadeia. Funciona camada por camada, começando pela camada de saída (final da rede) e retrocedendo em direção à entrada.</p>
<p>Em conjunto eles, temos a seguinte sequencia</p>
<ol type="1">
<li><p>No primeiro passo, os dados são introduzidos pela primeira vez na rede, onde os pesos e vieses são inicializaods de maneira aleatória. Ao final da primeira iteração, as predições iniciais são retornadas, possibilitando a utilização da função de custo</p></li>
<li><p>O backpropagation calcula o quanto uma variação nos pesos e vieses afetará a função de custo</p></li>
<li><p>O Gradiente Descendente atualiza os valores de pesos e vieses baseado nesses gradientes, buscando o valor que minimiza a função de perda</p></li>
<li><p>Se repete o ciclo por múltiplas iterações, chamadas de epochs até que a função de custo convirja para o mínimo global</p></li>
</ol>
<section id="exemplo" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="exemplo"><span class="header-section-number">7.1</span> Exemplo</h2>
<p>O seguinte exemplo foi construido, simulando o treinamento de uma rede via gradiente descendente e backpropagation</p>
<p>Com os seguintes</p>
<p>Net - Total de entrada de um neuronio</p>
<p>Out - Total de sáida de um neuronio</p>
<p>w_i_k - peso relacionado a saída e entrada k</p>
<p>Dado uma rede com 3 inputs, 1 camada oculta possuindo 2 neuronios e 1 neuronio na camada de saída, com função de ativação sigmoide e uma função de custo EQM</p>
<p><img src="images/example_ft1.png" class="img-fluid"> É importante ressaltar a nomenclatura utilizada, <span class="math inline">\(i\)</span> refere-se a os inputs, <span class="math inline">\(h\)</span> aos neurônios na camada oculta e <span class="math inline">\(o\)</span> ao neurônio na camada de saída. Para os pesos, de forma generalizada um <span class="math inline">\(w_{xyz}\)</span> representa um peso atrelado x-ésimo neuronio de uma camada anterior, ao y-ésimo neurônio z-ésima camada</p>
</section>
<section id="definindo-os-valores" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="definindo-os-valores"><span class="header-section-number">7.2</span> Definindo os Valores</h2>
<p><img src="images/example_ft2.png" class="img-fluid"></p>
<p><img src="images/example_ft3.png" class="img-fluid"></p>
<section id="forward-pass" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="forward-pass"><span class="header-section-number">7.2.1</span> Forward Pass</h3>
<p>Para começar, precisamos da predição inicial da rede dado os pesos e vieses aleatórios de inicialização</p>
</section>
<section id="calculando-o-erro-total" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="calculando-o-erro-total"><span class="header-section-number">7.2.2</span> Calculando o Erro Total</h3>
</section>
<section id="backward-pass" class="level3" data-number="7.2.3">
<h3 data-number="7.2.3" class="anchored" data-anchor-id="backward-pass"><span class="header-section-number">7.2.3</span> Backward Pass</h3>
<p>!. Camada de Saída</p>
<ol start="2" type="1">
<li>Camada Oculta</li>
</ol>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./GD.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Regressão Linear e Gradiente Descendente</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./summary.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Summary</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>