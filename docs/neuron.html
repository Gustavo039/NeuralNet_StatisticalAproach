<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Redes Neurais Artificiais: um bom lugar para um estatístico se deitar - 2&nbsp; Neuronios e Camadas</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./hyper.html" rel="next">
<link href="./GD.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Neuronios e Camadas</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Redes Neurais Artificiais: um bom lugar para um estatístico se deitar</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Prefácio</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./GD.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Regressão Linear e Gradiente Descendente</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./neuron.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Neuronios e Camadas</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hyper.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hiperparâmetros e Otimização</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#estrutura-básica" id="toc-estrutura-básica" class="nav-link active" data-scroll-target="#estrutura-básica"><span class="toc-section-number">3</span>  Estrutura Básica</a></li>
  <li><a href="#camadas" id="toc-camadas" class="nav-link" data-scroll-target="#camadas"><span class="toc-section-number">4</span>  Camadas</a></li>
  <li><a href="#função-de-ativação" id="toc-função-de-ativação" class="nav-link" data-scroll-target="#função-de-ativação"><span class="toc-section-number">5</span>  Função de Ativação</a></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation"><span class="toc-section-number">6</span>  Backpropagation</a>
  <ul class="collapse">
  <li><a href="#pesos-e-vieses" id="toc-pesos-e-vieses" class="nav-link" data-scroll-target="#pesos-e-vieses"><span class="toc-section-number">6.1</span>  Pesos e Vieses</a></li>
  <li><a href="#equações" id="toc-equações" class="nav-link" data-scroll-target="#equações"><span class="toc-section-number">6.2</span>  Equações</a>
  <ul class="collapse">
  <li><a href="#equação-para-o-erro-na-camada-de-saída" id="toc-equação-para-o-erro-na-camada-de-saída" class="nav-link" data-scroll-target="#equação-para-o-erro-na-camada-de-saída"><span class="toc-section-number">6.2.1</span>  Equação para o erro na camada de saída</a></li>
  <li><a href="#equação-para-o-erro" id="toc-equação-para-o-erro" class="nav-link" data-scroll-target="#equação-para-o-erro"><span class="toc-section-number">6.2.2</span>  Equação para o erro</a></li>
  <li><a href="#equação-para-a-taxa-de-variação-do-custo-em-relação-aos-vieses" id="toc-equação-para-a-taxa-de-variação-do-custo-em-relação-aos-vieses" class="nav-link" data-scroll-target="#equação-para-a-taxa-de-variação-do-custo-em-relação-aos-vieses"><span class="toc-section-number">6.2.3</span>  Equação para a taxa de variação do custo em relação aos vieses</a></li>
  <li><a href="#equação-para-a-taxa-de-variação-do-custo-em-relação-aos-pesos" id="toc-equação-para-a-taxa-de-variação-do-custo-em-relação-aos-pesos" class="nav-link" data-scroll-target="#equação-para-a-taxa-de-variação-do-custo-em-relação-aos-pesos"><span class="toc-section-number">6.2.4</span>  Equação para a taxa de variação do custo em relação aos pesos</a></li>
  </ul></li>
  <li><a href="#algoritmo" id="toc-algoritmo" class="nav-link" data-scroll-target="#algoritmo"><span class="toc-section-number">6.3</span>  Algoritmo</a></li>
  </ul></li>
  <li><a href="#gd-e-bck-em-conjunto" id="toc-gd-e-bck-em-conjunto" class="nav-link" data-scroll-target="#gd-e-bck-em-conjunto"><span class="toc-section-number">7</span>  GD e BCK em Conjunto</a>
  <ul class="collapse">
  <li><a href="#exemplo" id="toc-exemplo" class="nav-link" data-scroll-target="#exemplo"><span class="toc-section-number">7.1</span>  Exemplo</a></li>
  <li><a href="#definindo-os-valores" id="toc-definindo-os-valores" class="nav-link" data-scroll-target="#definindo-os-valores"><span class="toc-section-number">7.2</span>  Definindo os Valores</a></li>
  <li><a href="#forward-pass" id="toc-forward-pass" class="nav-link" data-scroll-target="#forward-pass"><span class="toc-section-number">7.3</span>  Forward Pass</a></li>
  <li><a href="#calculando-o-erro-total" id="toc-calculando-o-erro-total" class="nav-link" data-scroll-target="#calculando-o-erro-total"><span class="toc-section-number">7.4</span>  Calculando o Erro Total</a></li>
  <li><a href="#the-backwards-pass" id="toc-the-backwards-pass" class="nav-link" data-scroll-target="#the-backwards-pass"><span class="toc-section-number">7.5</span>  The Backwards Pass</a></li>
  </ul></li>
  <li><a href="#simulação-de-redes-neurais" id="toc-simulação-de-redes-neurais" class="nav-link" data-scroll-target="#simulação-de-redes-neurais"><span class="toc-section-number">8</span>  Simulação de Redes Neurais</a>
  <ul class="collapse">
  <li><a href="#dados-simulados" id="toc-dados-simulados" class="nav-link" data-scroll-target="#dados-simulados"><span class="toc-section-number">8.1</span>  Dados Simulados</a></li>
  <li><a href="#simulação-rede-slp" id="toc-simulação-rede-slp" class="nav-link" data-scroll-target="#simulação-rede-slp"><span class="toc-section-number">8.2</span>  Simulação Rede SLP</a></li>
  <li><a href="#simulação-rede-com-múltiplas-camadas-ocultas" id="toc-simulação-rede-com-múltiplas-camadas-ocultas" class="nav-link" data-scroll-target="#simulação-rede-com-múltiplas-camadas-ocultas"><span class="toc-section-number">8.3</span>  Simulação Rede com múltiplas camadas ocultas</a></li>
  </ul></li>
  <li><a href="#dados-brasileirão" id="toc-dados-brasileirão" class="nav-link" data-scroll-target="#dados-brasileirão"><span class="toc-section-number">9</span>  Dados Brasileirão</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Neuronios e Camadas</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Dado as definições iniciais das regressões linear e logistica, assim como do metodo de gradiente descendente, podemos passar para a definiçãop de uma Rede Neural. Para um bom entendimento de como funciona uma rede, destricharemos em 3 etapas:</p>
<ul>
<li>Arquitetura</li>
<li>Tipos</li>
<li>Aplicação</li>
</ul>
<p>Um neurônio em uma rede neural artificial é uma unidade computacional inspirada no funcionamento de um neurônio biológico. Ele recebe múltiplas entradas (inputs), realiza uma operação matemática para processar esses valores e gera uma saída (output). Essa operação geralmente envolve uma soma ponderada das entradas seguida por uma função de ativação, que transforma o valor resultante antes de enviá-lo à próxima camada da rede.</p>
<p>O neurônio humano é uma célula especializada no sistema nervoso, composta por dendritos, corpo celular (soma), axônio e terminais do axônio (sinapses). Os dendritos recebem sinais químicos e elétricos de outros neurônios, que se acumulam no corpo celular, onde esses sinais são integrados. Se a soma desses sinais ultrapassar um certo limiar, o neurônio gera um impulso elétrico, conhecido como potencial de ação. Esse impulso viaja pelo axônio até os terminais, onde é convertido novamente em sinal químico. Nessa etapa, neurotransmissores são liberados para se comunicar com outros neurônios através das sinapses, formando uma complexa rede de comunicação e processamento. Cada neurônio humano possui milhares de conexões (sinapses) com outros neurônios, o que permite um processamento de informações altamente paralelo e dinâmico, com adaptações complexas que envolvem neuroplasticidade ao longo do tempo.</p>
<p>Já um neurônio computacional, usado em redes neurais artificiais, é uma representação simplificada do neurônio humano. Ele possui uma estrutura mais básica, composta por entradas, uma soma ponderada e uma função de ativação. Cada entrada do neurônio computacional tem um peso associado, que representa a importância dessa entrada para o resultado final. O neurônio computacional calcula a soma ponderada das entradas, aplica uma função de ativação para transformar o resultado e então gera uma saída. Diferente do neurônio biológico, que pode transmitir informações de maneira complexa e em várias direções, o neurônio computacional apenas encaminha sua saída para os neurônios da próxima camada da rede.</p>
<p>Basicamento, podemos representar uma neuronio humano e computacional da seguinte forma</p>
<p><img src="images/neuron.png" class="img-fluid"></p>
<p>Um conjunto de neurônios humanos interconectados é conhecido como rede de neurônios ou <strong>rede neural</strong>. No cérebro humano, essas redes formam complexas interconexões chamadas de circuitos neurais, que são responsáveis pelo processamento de informações e pela comunicação entre diferentes partes do sistema nervoso.</p>
<p>No contexto computacional um conjunto de neurônios também é chamado de uma rede neural.</p>
<section id="estrutura-básica" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Estrutura Básica</h1>
<p>Um neurónio é a estrura mais básica de uma rede neural. Ele recebe um valor, processa ele, e retorna outro valor que é passado para o neuronio seguinte.</p>
<p>Em uma linguagem matemática, seja um neurônio <span class="math inline">\(K\)</span> <span class="math inline">\(x_1,x_2, ....,x_m\)</span> variáveis, <span class="math inline">\(m+1\)</span> entradas (<em>inputs</em>) e um vetor de pesos <span class="math inline">\(w_1,w_2, ..., w_m\)</span>.</p>
<ol type="1">
<li>Bias Input</li>
</ol>
<ul>
<li>O input <span class="math inline">\(x_0\)</span> é definido como o valor constante <span class="math inline">\(+1\)</span>. Isso o torna o chamado <strong>Bias Input</strong>, que é utilizado para ajustar o valor de saida do neurônio independente dos damais valores de entrada. Esse termo permite que o neurônio retorne valores diferentes de 0 mesmo quando todos os valores de entrada são iguais a 0.</li>
</ul>
<ol start="2" type="1">
<li>Actual Inputs</li>
</ol>
<ul>
<li>Os valores de entradas remanescente, vindo das variáveis <span class="math inline">\(x_1,x_2, ...x_m\)</span> são chamadas de entradas verdadeiras (<strong>Actual Inputs</strong>), tendo seu próprio vetor de peso associado <span class="math inline">\(w_{k1}, w_{k2},..., w_{km}\)</span></li>
</ul>
<ol start="3" type="1">
<li>Soma Ponderada</li>
</ol>
<ul>
<li>A soma dos valores de entrada ponderadas pelos pesos é realizada, dada por:</li>
</ul>
<p><span class="math display">\[z_k = \phi(\sum^m_{j=0}w_{kj}x_j) + b\]</span></p>
</section>
<section id="camadas" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Camadas</h1>
<p>Em rede neural, o fluxo dos dados percorre os neuronios atraves de determinados caminhos, chamados de <strong>Camadas</strong>.</p>
<p>Uma camada (layer) pode ser caracterizada como um conjunto de neuronios, onde neuronios de diferentes camadas possuem comunicação, porem neuronios de uma mesma camadas não possuem nenhuma ligação</p>
<p><img src="images/simple_nnet.png" class="img-fluid"></p>
<p>Uma rede possui 3 camadas distintas: Entrada (Input), Oculta (Hidden) e Saida (Output)</p>
<ul>
<li>Camada de Entrada
<ul>
<li>Se caracteriza como a primeira camada de uma rede e é responsável em receber os dados do conjunto de dados utilizado. No caso de um conjunto de dados estruturado, possuindo uma organização padrão de variáveis em colunas e observações em linhas, cada variável seria atribuido a um neuronio distinto, ou seja, se um conjunto de dados possui 10 diferentes variaveis, serão necessários 10 diferentes neuronios na camada de entrada. No caso de conjunto de dados não estruturados como imagens e sons, cada neuronio pode receber um pixel ou um determinado intervalo do som. Vale destacar que uma rede sempre vai possuir uma e somente uma camada de entrada</li>
</ul></li>
<li>Camadas Ocultas
<ul>
<li>Se caracteriza como as camadas que estão entre a camada de entrada e a camadas de saída, e não possui um número fixo. Redes mais simples possuem entre 1 a 3 camadas ocultas. Para problemas mais complexos esse número aumenta. A definição do número de camadas ocultas pode se basear em 2 abordagems: Fixar ou Otimizar.</li>
<li>Fixar o número de camadas ocultas significa definir um valor fixo antes de iniciar o treinamento do modelo. Essa abordagem é baseada em escolhas prévias, geralmente fundamentadas no conhecimento do problema ou em heurísticas. Redes simples, que resolvem problemas com poucos dados ou baixa complexidade, muitas vezes utilizam entre 1 e 3 camadas ocultas. Esse método é vantajoso em cenários onde se busca simplicidade no design do modelo, menor custo computacional e rapidez no desenvolvimento.</li>
<li>Otimizar o número de camadas ocultas é uma abordagem que leva a observamos esse número como hiperparâmetro, e assim aplicar tecnicas de tuning para encontrar um número ótimo. Essa opção é utilizada quando não temos nenhum conhecimento a priori do preblema e redes que ja foram utilizadas em problemas semelhantes.</li>
</ul></li>
<li>Camada de Saida
<ul>
<li>É a última camada de uma rede e é responsável em passar os valores finais preditos. Toda rede vai possuir uma e somente uma camada de saída, onde o número de neurônios dessa camada vai depender do contexto do problema. Em um problema de classificação, devemos ter um neuronio para cada classe. Já ára um problema de regressão, devemos definir um neurinio para cada dimensão, ou seja para uma regressão com a predição de apenas um valor (univariada) devemos ter apenas 1 neurônio, ja para regressão com <span class="math inline">\(m\)</span> valores preditos (multivariada), devemos definir <span class="math inline">\(m\)</span> neurônios nessa camada</li>
</ul></li>
</ul>
</section>
<section id="função-de-ativação" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Função de Ativação</h1>
<p>Como definimos na seção “Estrutura Básica”, um neurônio é definido algebricamente como <span class="math display">\[z_k = \phi(\sum^m_{j=0}w_{kj}x_j) + b\]</span>, onde <span class="math inline">\(phi\)</span> é uma função chamada de função de ativação. Ela é responsável por transformar o valor bruto calculado por um neurônio (o somatório ponderado das entradas mais o viés) em uma saída que será transmitida para os neurônios da próxima camada. O principal objetivo dessa transformação é introduzir não-linearidade no modelo, permitindo que ele aprenda padrões complexos nos dados.</p>
<p>Sem uma função de ativação, a rede neural seria equivalente a uma combinação linear das entradas, independentemente de quantas camadas fossem adicionadas. Isso significa que ela só poderia resolver problemas simples e lineares. A introdução de não-linearidade torna possível a resolução de problemas mais desafiadores, como a classificação de dados que não podem ser separados por uma linha ou plano.</p>
<p>Atualmente, existem diversas funções de ativação já definidas, onde cada uma possui uma nicho específico de utilização. Listamos as mais conhecidas</p>
<ul>
<li><p>Sigmoide</p>
<ul>
<li><span class="math inline">\(f(x) = \frac{1}{1+e^{-x}}\)</span></li>
<li>Transforma a saída para o entre intervalo 0 e 1, sendo ideal para tarefas de classificação binária.</li>
<li>Extremamente popular em redes mais antigas, porém tem limitações como a saturação e o problema do “vanishing gradient”, fenômeno esse que se caracteriza quando a função de ativação gera gradientes tão pequenos, que diminuem ainda mais à medida que são propagados para camadas anteriores -&gt; Colocar foto da função, sua equação</li>
</ul></li>
<li><p>Tanh (Tangente Hiperbólica)</p>
<ul>
<li><span class="math inline">\(f(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}\)</span></li>
<li>Transforma a saída para o entre intervalo -1 e 1 -&gt; Colocar foto da função, sua equação</li>
</ul></li>
<li><p>ReLU (Rectified Linear Unit)</p>
<ul>
<li><span class="math inline">\(f(x) = \max(0,x)\)</span></li>
</ul></li>
<li><p>Leaky ReLu (ReLU Contaminada)</p>
<ul>
<li><p><span class="math inline">\(f(x) = \begin{cases} x, &amp; \text{se } x&gt;0 \\ \alpha x, &amp; \text{se } x\leq 0 \end{cases}\)</span></p></li>
<li><p><span class="math inline">\(\alpha\)</span> é uma constante que irar ponderar os valores de x menores ou iguais a 0, podendo ser fixada antes do treinamento do modelo, ou classificada como um parâmetro, passando pela etapa de otimização</p></li>
</ul></li>
<li><p>Softmax</p>
<ul>
<li><span class="math inline">\(\sigma(z_i) = \frac{e^{z_i}}{\sum^L_{j=i}e^{zj}}\)</span></li>
</ul></li>
</ul>
<p>Vale destacar que <strong>uma função de ativação e função de custo são coisas DISTINTAS</strong>. Enquanto uma função de ativação é aplicada localmente em cada neurônio, uma função de custo é aplicada apenas na sáida final da rede, ou seja, no momento da predição de valores, tendo o objetivo de calcular o erro global da rede. Essa função é utilizada para a otimização dos pesos e vieses durante o treinamento. Resumidamente, a função de ativação transforma os sinais dentro da rede para capturar padrões, enquanto a função de custo avalia o erro final das previsões para guiar o aprendizado do modelo.</p>
</section>
<section id="backpropagation" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Backpropagation</h1>
<p>O objetivo do Backpropagation é calcular as derivadas parciais <span class="math inline">\(\frac{\partial C}{\partial w}\)</span> e <span class="math inline">\(\frac{\partial C}{\partial b}\)</span>, onde <span class="math inline">\(C\)</span> é função de custo, <span class="math inline">\(w\)</span> é o peso (weight) e <span class="math inline">\(b\)</span> é o viés (bias). Para o método funcionar, precisamos definir duas suposições</p>
<ol type="1">
<li>Generalização pela media</li>
</ol>
<ul>
<li>A função de custo <span class="math inline">\(C\)</span> pode ser reescrita como <span class="math inline">\(C = \frac{1}{n}\sum_xC_x\)</span>. Isso deve ser assumido por conta da forma que o método calcula as derivadas parciais <span class="math inline">\(\frac{\partial C}{\partial w}\)</span> e <span class="math inline">\(\frac{\partial C}{\partial b}\)</span>, onde dado <span class="math inline">\(x\)</span> o conjunto de dados de treinamento da iteração, temos na verdade as derivadas <span class="math inline">\(\frac{\partial C_x}{\partial w}\)</span> e <span class="math inline">\(\frac{\partial C_x}{\partial b}\)</span>.</li>
</ul>
<ol start="2" type="1">
<li>Função de custo pode ser reescrita como uma função da saída (output) da rede</li>
</ol>
<ul>
<li>Dado os neuronios da camada de saida <span class="math inline">\(k^L_1, k^L_2, ..., k^L_j\)</span>, e seus outputs <span class="math inline">\(a^L_1, a^L_2, ..., a^L_j\)</span>, a função de custo <span class="math inline">\(C\)</span> pode ser reescrita como <span class="math inline">\(C = C(a^l)\)</span></li>
</ul>
<section id="pesos-e-vieses" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="pesos-e-vieses"><span class="header-section-number">6.1</span> Pesos e Vieses</h2>
<p>Antes de entrar de fato nas equações e formas do método, devemos fazer definições importantes sobre a nomenclatura de certos parâmetros e suas caracteristicas</p>
<p>Quando falamos de pesos e vieses, há muita confusão sobre a função e aplicação de cada um, onde muitas pessoas utilizam esses termos como sinônimos, o que fortemente não é verdade.</p>
<p>A confusão entre pesos e vieses surge frequentemente porque ambos são parâmetros de uma rede neural que são ajustados durante o treinamento e determinam coletivamente o comportamento do modelo. No entanto, eles servem a propósitos e funções distintas.</p>
<p>Os pesos (weights) são fatores de <strong>multiplicação</strong> aplicados aos valores em cada neuronio e determinam a força e direção da relação entre os valores de entrada e de saída de um neurônio.</p>
<p>Já os vieses (bias) são fatores de <strong>soma</strong> e permitem o deslocamento da função de ativação, auxiliando que o modelo se ajuste melhor aos dados</p>
<p>Dado uma rede com uma função de ativação <span class="math inline">\(At\)</span>, temos o seguinte valor de saida de um determinado neurônio</p>
<p><span class="math display">\[z = At(w_i* x_i) + b\]</span> Vemos que o peso <span class="math inline">\(w_i\)</span> multiplica o valor de entrada <span class="math inline">\(x_i\)</span>, controlando a influência de <span class="math inline">\(x_i\)</span> no neuronio atual. Já o vies <span class="math inline">\(b\)</span> é somado ao valor retornado pela função de ativação, auxiliando o neuronio a melhor se ajustar aos dados</p>
<p>As principais diferenças podem resumidas em uma tabela</p>
</section>
<section id="equações" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="equações"><span class="header-section-number">6.2</span> Equações</h2>
<p>O método se baseia em 4 equações fundamentais</p>
<section id="equação-para-o-erro-na-camada-de-saída" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="equação-para-o-erro-na-camada-de-saída"><span class="header-section-number">6.2.1</span> Equação para o erro na camada de saída</h3>
<p><span class="math display">\[\delta^L_j = \frac{\partial C}{\partial a^L_j}\sigma´(z^L_j)\]</span></p>
<p>O primeiro termo a direta, <span class="math inline">\(\frac{\partial C}{\partial a^L_j}\)</span> mensura o quão rápido a função de custo está se adaptando em relação ao j-ésimo neurônio de saída. Por exemplo, se a função custo não depender muito de um neuronio j em particular, portanto <span class="math inline">\(\delta^L_j\)</span> será um valor pequeno</p>
<p>Já o segundo termo a direita, <span class="math inline">\(\sigma´(z^L_j)\)</span>, mensura o quão rápido a função de ativação <span class="math inline">\(\sigma\)</span> esta mudando em relação a <span class="math inline">\(z^L_j\)</span></p>
<p>Na forma matricial a BP1 possui a seguinte forma</p>
<p><span class="math display">\[\sigma^L = \Delta_aC \odot \sigma´ (z^L)\]</span> Onde, <span class="math inline">\(\Delta_aC\)</span> é definido como o vetor que sias componentes são as derivadas parciais <span class="math inline">\(\frac{\partial C}{\partial a^L_j}\)</span>, para facilitar o entedimento, podemos expressar <span class="math inline">\(\Delta_aC\)</span> como a taxa de variação de <span class="math inline">\(C\)</span> em relação ao ativações de saída</p>
</section>
<section id="equação-para-o-erro" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="equação-para-o-erro"><span class="header-section-number">6.2.2</span> Equação para o erro</h3>
<p>A equação para o erro <span class="math inline">\(\delta^l\)</span> em relação ao erro uam camada a frente, <span class="math inline">\(\delta^{l+1}\)</span> é dado por</p>
<p><span class="math display">\[\delta^l = ((w^{l+1})^T\delta^{l+1})\odot\sigma´(z^l)\]</span></p>
<p>Onde <span class="math inline">\((w^{l+1})^T\)</span> é a matriz transposta da matriz de pesos <span class="math inline">\(w^{l+1}\)</span> para a <span class="math inline">\(l+1\)</span>-ésima camada</p>
</section>
<section id="equação-para-a-taxa-de-variação-do-custo-em-relação-aos-vieses" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="equação-para-a-taxa-de-variação-do-custo-em-relação-aos-vieses"><span class="header-section-number">6.2.3</span> Equação para a taxa de variação do custo em relação aos vieses</h3>
<p><span id="eq-bp1"><span class="math display">\[
\frac{\partial C}{\partial b^l_j} = \delta^l_j
\tag{6.1}\]</span></span></p>
<p>Temos que o erro <span class="math inline">\(\delta^l_j\)</span> é exatamente igual a taxa de variação <span class="math inline">\(\frac{\partial C}{\partial b^l_j}\)</span>. Isso se mostra como um ponto positivo, dado que já sabemos como calcular <span class="math inline">\(\delta^l_j\)</span> como visto nas equações BP1 e BP2. Assim podemos escrever a BP3 como</p>
<p><span class="math display">\[\frac{\partial C}{\partial b} = \delta\]</span></p>
<p>Onde <span class="math inline">\(\delta\)</span> está sendo calculado no mesmo neuronio do viés <span class="math inline">\(b\)</span></p>
</section>
<section id="equação-para-a-taxa-de-variação-do-custo-em-relação-aos-pesos" class="level3" data-number="6.2.4">
<h3 data-number="6.2.4" class="anchored" data-anchor-id="equação-para-a-taxa-de-variação-do-custo-em-relação-aos-pesos"><span class="header-section-number">6.2.4</span> Equação para a taxa de variação do custo em relação aos pesos</h3>
<p><span class="math display">\[\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k\delta^l_j\]</span></p>
</section>
</section>
<section id="algoritmo" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="algoritmo"><span class="header-section-number">6.3</span> Algoritmo</h2>
<p>As funções definidas na seção passada provem uma forma de se calcular o gradiente da função de custo. Essas funções são utilizadas no algortimo do método que possui 5 passos fundamentais</p>
<ol type="1">
<li>Input x</li>
</ol>
<ul>
<li>O conjunto dados é introduzido a rede. No pensamento algebrico, o vetor <span class="math inline">\(X\)</span> é introduzido a rede e atribuido como a ativação da camada de entrada</li>
</ul>
<ol start="2" type="1">
<li>Feedforward</li>
</ol>
<ul>
<li>Para cada camada <span class="math inline">\(l = 2,3, ..., L\)</span> calcula-se os valores retornado pelos neurônios <span class="math inline">\(z^l = w^la^{l-1}+b^l\)</span>, onde <span class="math inline">\(w^l\)</span> é a matriz de peso para a camada <span class="math inline">\(l\)</span>, <span class="math inline">\(a^{l-1}\)</span> é o valor de ativação da camada anterior, <span class="math inline">\(b^l\)</span> é o vetor de vieses para a camada <span class="math inline">\(l\)</span></li>
<li>Além disso, devemos reforçar que <span class="math inline">\(a^l = \sigma(z^l)\)</span>, onde <span class="math inline">\(\sigma\)</span> é uma função de ativação</li>
</ul>
<ol start="3" type="1">
<li>Erro de Saída</li>
</ol>
<ul>
<li>O erro na camada de saída é calculado, dado por <span class="math display">\[\delta^L=\Delta_aC\odot\sigma´(z^L)\]</span></li>
<li><span class="math inline">\(\Delta_aC\)</span> é o gradiente da função de custo C em relação ao valor de ativação <span class="math inline">\(a^L\)</span></li>
<li><span class="math inline">\(\sigma´(z^L)\)</span> é a derivada da função de ligação no valor de ativação <span class="math inline">\(z^l\)</span></li>
<li>Isso quantifica o quanto a saída da rede (ativações) se desvia da saída desejada.</li>
</ul>
<ol start="4" type="1">
<li>Propagação do Erro</li>
</ol>
<ul>
<li>Para cada camada <span class="math inline">\(l = L-1,L-2, ...,2\)</span>, o erro para a camada é calculado, dado por <span class="math display">\[\delta^l = ((w^{l+1})^T\delta^{l+1})\odot\sigma´(z^l)\]</span></li>
<li><span class="math inline">\((w^{l+1})^T\delta^{l+1}\)</span> propaga o erro para trás na rede</li>
<li><span class="math inline">\(\sigma´(z^l)\)</span> ajusta o erro baseado na função de ativação</li>
<li>Nessa etapa o erro é propagado para trás, dando origem ao nome do método <strong>Backpropagation</strong></li>
</ul>
<ol start="5" type="1">
<li>Calculo dos Gradientes</li>
</ol>
<ul>
<li>No último passo do algoritmo, os gradientes em relação aos pesos e vieses são calculados</li>
<li>Para o peso (weight) temos <span class="math display">\[\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k\delta^l_j\]</span></li>
<li>Já para os vieses (bias) <span class="math display">\[\frac{\partial C}{\partial b^l_{j}} = \delta^l_j\]</span></li>
<li>Os valores calculados para cada gradiente são utilizados no momento da otimização via gradiente descendente, minimizando a função de custo</li>
</ul>
</section>
</section>
<section id="gd-e-bck-em-conjunto" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> GD e BCK em Conjunto</h1>
<p>O GD e BCK fazem um trabalho de turma</p>
<p>Relembrando, Gradient Descent é um algoritmo usado para minimizar uma função, neste caso, a função de custo <span class="math inline">\(C(\theta)\)</span>, onde <span class="math inline">\(\theta\)</span> representa a matriz de parâmetros: pesos e vieses</p>
<ul>
<li>O algoritmo atualiza os parâmetros iterativamente na direção do gradiente negativo da perda em relação aos parâmetros: <span class="math display">\[\theta^{(t+1)} = \theta(t) -\eta\nabla J(\theta^{(t)})\]</span></li>
</ul>
<p>O desafio mora no calculo do gradiente <span class="math inline">\(\nabla_{\theta}L(\theta)\)</span>, que é aí que se utiliza o Backpropagation</p>
<p>Backpropagation é um algoritmo para calcular os gradientes da função de custo em relação aos pesos e vieses da rede de forma eficiente usando a regra da cadeia. Funciona camada por camada, começando pela camada de saída (final da rede) e retrocedendo em direção à entrada.</p>
<p>Em conjunto eles, temos a seguinte sequencia</p>
<ol type="1">
<li><p>No primeiro passo, os dados são introduzidos pela primeira vez na rede, onde os pesos e vieses são inicializaods de maneira aleatória. Ao final da primeira iteração, as predições iniciais são retornadas, possibilitando a utilização da função de custo</p></li>
<li><p>O backpropagation calcula o quanto uma variação nos pesos e vieses afetará a função de custo</p></li>
<li><p>O Gradiente Descendente atualiza os valores de pesos e vieses baseado nesses gradientes, buscando o valor que minimiza a função de perda</p></li>
<li><p>Se repete o ciclo por múltiplas iterações, chamadas de epochs até que a função de custo convirja para o mínimo global</p></li>
</ol>
<section id="exemplo" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="exemplo"><span class="header-section-number">7.1</span> Exemplo</h2>
<p>O exemplo a seguir foi desenvolvido para ilustrar o treinamento de uma rede neural utilizando os algoritmos de Gradiente Descendente e Backpropagation.</p>
<p>O problema abordado é do tipo regressão, com o objetivo de prever um valor numérico. A estrutura da rede foi definida da seguinte maneira: a camada de entrada possui 3 features, a camada oculta contém 2 neurônios, e a camada de saída é composta por 1 neurônio, responsável por gerar a predição final.</p>
<p>A função de ativação utilizada é a <strong>ReLU</strong> com a função de custo <strong>Erro Quadrático Médio</strong></p>
<p>Para facilitar os calculos, utilizaremos a seguinte nomenclatura para um neuronio</p>
<p><img src="images/example_ft4.png" class="img-fluid"></p>
<p>O funcionamento de um neurônio pode ser dividido em duas etapas principais. A primeira etapa corresponde à <strong>soma ponderada dos valores de entrada</strong>, calculada como <span class="math inline">\(Net = \sum_{i=1}^Ix_iw_i\)</span>, onde <span class="math inline">\(x_i\)</span> são os valores da entrada e <span class="math inline">\(w_i\)</span> os respectivos pesos, e <strong>Net</strong> representa o resultado dessa operação</p>
<p>A segunda etapa consiste na <strong>aplicação da função de ativação</strong> ao valor retornado por Net, gerando a saída do neurônio. Esse processo é expresso como <span class="math inline">\(Out = Fa(Net) + Bias\)</span>, onde <span class="math inline">\(Fa\)</span> é a função de ativação escolhida e o termo Bias ajusta o resultado final</p>
<p>Essa separação entre as etapas de cálculo da soma ponderada (Net) e a aplicação da função de ativação (Out) é crucial para a aplicação da regra da cadeia durante o cálculo dos gradientes na etapa de backpropagation.</p>
<p>Conceitualmente, a rede possui a seguinte arquitetura</p>
<p><img src="images/example_ft1.png" class="img-fluid"> É importante ressaltar a nomenclatura utilizada, <span class="math inline">\(i\)</span> refere-se a os inputs, <span class="math inline">\(h\)</span> aos neurônios na camada oculta e <span class="math inline">\(o\)</span> ao neurônio na camada de saída. Para os pesos, de forma generalizada um <span class="math inline">\(w_{xyz}\)</span> representa um peso atrelado x-ésimo neuronio de uma camada anterior, ao y-ésimo neurônio z-ésima camada, onde a rede possui um número de camadas indo de 0 (camada de entrada) a L (camada oculta). Por exemplo, um peso <span class="math inline">\(w_212\)</span> representa um peso que liga o segundo neuronio da camada 1 ao primeiro neuronio da camada 2.</p>
</section>
<section id="definindo-os-valores" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="definindo-os-valores"><span class="header-section-number">7.2</span> Definindo os Valores</h2>
<p>Para o exemplo,</p>
<p><img src="images/example_ft2.png" class="img-fluid"></p>
<p>O valor verdadeiro é <strong>11</strong></p>
<p><img src="images/example_ft3.png" class="img-fluid"></p>
</section>
<section id="forward-pass" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="forward-pass"><span class="header-section-number">7.3</span> Forward Pass</h2>
<p>Para começar, precisamos da predição inicial da rede dado os pesos e vieses aleatórios de inicialização.</p>
<ol type="1">
<li>Camada Oculta</li>
</ol>
<p>O primeiro passo é calcular os valores passados para a camada oculta, calculando os valores de Net e Out</p>
<p>Para <span class="math inline">\(h_1\)</span> temos</p>
<p><span class="math display">\[Net_{h_1} = i_1w_{111} + i_2w_{211} + i_3w_{311}\]</span> <span class="math display">\[Net_{h_1} = 3*0.5 + 9*0.6 + 21*0.4 = 15.5\]</span> <span class="math display">\[Out_{h_1} = Fa(Net_{h_1}) + Bias_1\]</span> <span class="math display">\[Out_{h_1} = max(0;15.5) + 0.5 = 16\]</span></p>
<p>Repetindo os mesmos passos para <span class="math inline">\(h_2\)</span></p>
<p><span class="math display">\[Net_{h_2} = i_1w_{121} + i_2w_{221} + i_3w_{321}\]</span> <span class="math display">\[Net_{h_2} = 3*0.7 + 9*0.5 + 21*0.6 = 19.2\]</span> <span class="math display">\[Out_{h_2} = Fa(Net_{h_2}) + Bias_1\]</span> <span class="math display">\[Out_{h_2} = max(0;19.2) + 0.5 = 19.7\]</span></p>
<p>Com valores de <span class="math inline">\(h_1\)</span> e <span class="math inline">\(h_2\)</span> calculados, podemos calcular o valor de <span class="math inline">\(o_1\)</span></p>
<p><span class="math display">\[Net_{o_1} = h_1w_{112} + h_2w_{212}\]</span> <span class="math display">\[Net_{o_1} = 16 * 0.4 + 19.7 * 0.9 = 24.1\]</span> <span class="math display">\[Out_{o_1} = Fa(Net_{o_1}) + Bias_2\]</span> <span class="math display">\[Out_{o_1} = max(0;24.1) + 0.3 = 24.4\]</span></p>
</section>
<section id="calculando-o-erro-total" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="calculando-o-erro-total"><span class="header-section-number">7.4</span> Calculando o Erro Total</h2>
<p>Com o valor predito pelo modelo, podemos calcular o Erro, lembrando que o valor verdadeiro é <strong>11</strong>. Para o erro, a função de custo utilizada é o <strong>Erro Quadrático Médio</strong>, dado por</p>
<p><span class="math display">\[E_{total} = \sum\frac{1}{2}(y-\hat y)^2\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Dica
</div>
</div>
<div class="callout-body-container callout-body">
<p>O <span class="math inline">\(\frac{1}{2}\)</span> é incluido para que o expoente seja cancelado durante o calculado da derivada.</p>
</div>
</div>
<p>Assim, temos o seguinte erro</p>
<p><span class="math display">\[E_{total} = \frac{1}{2}(11-24.4)^2 = 89.8\]</span></p>
</section>
<section id="the-backwards-pass" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="the-backwards-pass"><span class="header-section-number">7.5</span> The Backwards Pass</h2>
<p>O objetivo do backpropagation é utilizar o erro calculado na saída da rede para ajustar os valores dos pesos e vieses de maneira eficiente. Esse ajuste é feito iterativamente, com o intuito de minimizar o erro ao longo do processo de treinamento, levando a uma melhoria contínua no desempenho da rede. Em essência, o backpropagation aplica a regra da cadeia para propagar o erro da camada de saída até as camadas iniciais, identificando como cada peso e viés contribui para o erro, e, em seguida, atualizando esses parâmetros para reduzir gradativamente o valor do erro.</p>
<p>Para a atualização dos valores dos pesos e vieses, utilizamos o seguinte método:</p>
<p>Para atualizar o valor de um certo peso, devemos calcular sua constribuição para o erro do total da rede. Por exemplo, se estamos trabalhando com o peso <span class="math inline">\(w_{112}\)</span>, sua contribuição é dado pela seguinte derivada</p>
<p><span class="math display">\[\frac{\partial E_{total}}{\partial w_{112}}\]</span></p>
<p>O cálculo dessa derivada é realizada através da regra da cadeia</p>
<p><span class="math display">\[\frac{\partial E_{total}}{\partial w_{112}} =
\frac{\partial E_{total}}{\partial out_{o_{1}}}
\times
\frac{\partial out_{o_{1}}}{\partial net_{o_{1}}}
\times
\frac{\partial net_{o_{1}}}{\partial w_{112}}\]</span></p>
<p>Para o cálculo dessas derivadas, devemos antes definir claramente cada função.</p>
<ul>
<li><span class="math inline">\(E_{total}\)</span>
<ul>
<li>É a função de custo definida para cada rede. No exemplo estudado, a função de custo definida foi o <strong>EQM</strong>, dada por <span class="math inline">\(E_{total} = \sum\frac{1}{2}(y-\hat y)^2 = \sum\frac{1}{2}(y-out_{o_1})^2\)</span></li>
</ul></li>
<li><span class="math inline">\(Out_{o}\)</span>
<ul>
<li>A função <strong>Out</strong> de um determinado neurônio são os valores retornado por <strong>Net</strong> aplicado na função de ativação, dado por <span class="math inline">\(Out_{} = Fa(Net) + Bias\)</span></li>
<li>Para o exemplo, <span class="math inline">\(Out_{o_1} = ReLU(Net_{o_1}) + Bias_2\)</span></li>
</ul></li>
<li><span class="math inline">\(Net_{o}\)</span>
<ul>
<li>A função <strong>Net</strong> é etapa da soma ponderada, onde os valores</li>
<li>Para o exemplo, <span class="math inline">\(Net_{o_1} = Out_{h_1}w_{112} + Out_{h_2}w_{212}\)</span></li>
</ul></li>
</ul>
<p>Com cada função devidamente definida, podemos calcular as derivadas parciais</p>
<p><span class="math display">\[\frac{\partial E_{total}}{\partial out_{o_{1}}}= -(y-out_{o_1})\]</span></p>
<p><span class="math display">\[\frac{\partial out_{o_{1}}}{\partial net_{o_{1}}} = \begin{cases}
0 &amp; \text{se } net_{o_1} &lt; 0, \\
1 &amp; \text{se } net_{o_1} &gt; 0.
\end{cases}\]</span></p>
<p><span class="math display">\[\frac{\partial net_{o_{1}}}{\partial w_{112}} = Out_{h_1}\]</span></p>
<p>Assim</p>
<p><span class="math display">\[\frac{\partial E_{total}}{\partial w_{112}} = -(y-out_{o_1}) \times \begin{cases}
0 &amp; \text{se } net_{o_1} &lt; 0, \\
1 &amp; \text{se } net_{o_1} &gt; 0
\end{cases} \times Out_{h_1}  = -(11 - 24.4) \times 1 \times 16 = 214.4\]</span></p>
<p>Com o valor calculado, podemos finalmente atualizar o valor de <span class="math inline">\(w_{112}\)</span>. A atualização é dada por</p>
<p><span class="math display">\[w_{112}^+ = w_{112} - \frac{\partial E_{total}}{\partial w_{112}} = 0.4 - 214.4 = -214\]</span></p>
<p>Veremos mais pra frente o conceito de tunagem de hiperparametros, onde um deles é a taxa de aprendizado (<em>learning rate</em>), esse parametro é utilizado para ajustar a magnitude do valor atualizado por iteração. Utilizando ele, a atualização é dada por <span class="math display">\[w_{112}^+ = w_{112} - \alpha \frac{\partial E_{total}}{\partial w_{112}}\]</span> onde <span class="math inline">\(\alpha\)</span> é um valor que deve ser tunado (processo semelhante a otimização)</p>
</section>
</section>
<section id="simulação-de-redes-neurais" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Simulação de Redes Neurais</h1>
<p>Agora que já entendemos o funcionamento matemático de uma rede neural feedforward, vamos colocar a teoria em prática com simulações em R. O objetivo é:</p>
<ol type="1">
<li><p>Criar uma rede simples, com apenas uma camada oculta (SLP — Single Layer Perceptron).</p></li>
<li><p>Compará-la com modelos de regressão linear.</p></li>
<li><p>Generalizar o código para redes com múltiplas camadas ocultas e compará-las novamente.</p></li>
</ol>
<hr>
<section id="dados-simulados" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="dados-simulados"><span class="header-section-number">8.1</span> Dados Simulados</h2>
<p>1.1 Preparanda a simulação</p>
<p>Começaremos gerando um conjunto de dados sintético, onde a variável resposta <span class="math inline">\(y\)</span> é uma combinação linear de três variáveis <span class="math inline">\(x_1,x_2,x_3\)</span> mais um erro aleatório (que segue uma distribuição Normal(0,1))</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>beta_0 <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>beta_2 <span class="ot">=</span> <span class="fl">0.02</span> </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>beta_3 <span class="ot">=</span> <span class="dv">55</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>data_nnet <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_1 =</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">3</span>),</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_2 =</span> <span class="fu">rexp</span>(<span class="dv">100</span>, <span class="at">rate =</span> <span class="dv">5</span>),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_3 =</span> <span class="fu">rgamma</span>(<span class="dv">100</span>, <span class="at">shape =</span> <span class="dv">1</span>, <span class="at">rate =</span> <span class="fl">0.4</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>) <span class="sc">|&gt;</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> beta_0 <span class="sc">+</span> (beta_1<span class="sc">*</span>x_1) <span class="sc">+</span> (beta_2<span class="sc">*</span>x_2) <span class="sc">+</span> (beta_3<span class="sc">*</span>x_3) <span class="sc">+</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>      <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="simulação-rede-slp" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="simulação-rede-slp"><span class="header-section-number">8.2</span> Simulação Rede SLP</h2>
<p>Para a primeira rodada de simulação, utilizamos uma rede simples, com apenas uma camada oculta (uma single layer perceptron), com as seguintes definições</p>
<ul>
<li><p>1 camada oculta</p></li>
<li><p>2 neurônios</p></li>
<li><p>Função de ativação: ReLU</p></li>
<li><p>Função de custo: Erro Quadrático Médio (EQM)</p></li>
</ul>
<p>Para a construção e treinamento da rede, as seguintes funções foram construídas. É importante perceber que tais funções servem apenas para a construção de redes com 1 camada oculta. Mais adinate veremos o cófigo utilizado para redes com mais 1 camada oculta</p>
<p><strong>Funções básicas</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Função de ativação ReLU</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>relu <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">pmax</span>(<span class="dv">0</span>, x)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>relu_deriv <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Função de custo EQM</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>eqm <span class="ot">=</span> <span class="cf">function</span>(y, y_hat) (y<span class="sc">-</span>y_hat)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>eqm_deriv <span class="ot">=</span> <span class="cf">function</span>(y, y_hat) <span class="sc">-</span>(y<span class="sc">-</span>y_hat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Inicialização da rede</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>init_network <span class="ot">=</span> <span class="cf">function</span>(input_size, hidden_size, output_size) {</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">W1 =</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(input_size <span class="sc">*</span> hidden_size, <span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="at">nrow =</span> hidden_size),</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">b1 =</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> hidden_size, <span class="at">ncol =</span> <span class="dv">1</span>),</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">W2 =</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(hidden_size <span class="sc">*</span> output_size, <span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="at">nrow =</span> output_size),</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">b2 =</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> output_size, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Passo forward e backward</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward e backward</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>forward_backward <span class="ot">=</span> <span class="cf">function</span>(network, x, y) {</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  z1 <span class="ot">=</span> network<span class="sc">$</span>W1 <span class="sc">%*%</span> x <span class="sc">+</span> network<span class="sc">$</span>b1</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  a1 <span class="ot">=</span> <span class="fu">relu</span>(z1)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  z2 <span class="ot">=</span> network<span class="sc">$</span>W2 <span class="sc">%*%</span> a1 <span class="sc">+</span> network<span class="sc">$</span>b2</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">=</span> z2</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Erro e perda</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">mean</span>((y_pred <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backward (gradientes)</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>  dL_dz2 <span class="ot">=</span> <span class="dv">2</span> <span class="sc">*</span> (y_pred <span class="sc">-</span> y)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  dL_dW2 <span class="ot">=</span> dL_dz2 <span class="sc">%*%</span> <span class="fu">t</span>(a1)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  dL_db2 <span class="ot">=</span> dL_dz2</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>  dL_da1 <span class="ot">=</span> <span class="fu">t</span>(network<span class="sc">$</span>W2) <span class="sc">%*%</span> dL_dz2</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>  dL_dz1 <span class="ot">=</span> dL_da1 <span class="sc">*</span> <span class="fu">relu_deriv</span>(z1)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>  dL_dW1 <span class="ot">=</span> dL_dz1 <span class="sc">%*%</span> <span class="fu">t</span>(x)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>  dL_db1 <span class="ot">=</span> dL_dz1</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> loss,</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">gradients =</span> <span class="fu">list</span>(<span class="at">W1 =</span> dL_dW1, <span class="at">b1 =</span> dL_db1, <span class="at">W2 =</span> dL_dW2, <span class="at">b2 =</span> dL_db2)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Treinamento</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Treinamento</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>train_nn <span class="ot">=</span> <span class="cf">function</span>(X, Y, <span class="at">hidden_size =</span> <span class="dv">5</span>, <span class="at">epochs =</span> <span class="dv">1000</span>, <span class="at">lr =</span> <span class="fl">0.01</span>) {</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">t</span>(X)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  Y <span class="ot">=</span> <span class="fu">t</span>(Y)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  input_size <span class="ot">=</span> <span class="fu">nrow</span>(X)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  output_size <span class="ot">=</span> <span class="fu">nrow</span>(Y)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  network <span class="ot">=</span> <span class="fu">init_network</span>(input_size, hidden_size, output_size)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (epoch <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>epochs) {</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(X)) {</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>      x_i <span class="ot">=</span> X[, i, drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>      y_i <span class="ot">=</span> Y[, i, drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>      fb <span class="ot">=</span> <span class="fu">forward_backward</span>(network, x_i, y_i)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>      total_loss <span class="ot">=</span> total_loss <span class="sc">+</span> fb<span class="sc">$</span>loss</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Atualização dos pesos</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>      network<span class="sc">$</span>W1 <span class="ot">=</span> network<span class="sc">$</span>W1 <span class="sc">-</span> lr <span class="sc">*</span> fb<span class="sc">$</span>gradients<span class="sc">$</span>W1</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>      network<span class="sc">$</span>b1 <span class="ot">=</span> network<span class="sc">$</span>b1 <span class="sc">-</span> lr <span class="sc">*</span> fb<span class="sc">$</span>gradients<span class="sc">$</span>b1</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>      network<span class="sc">$</span>W2 <span class="ot">=</span> network<span class="sc">$</span>W2 <span class="sc">-</span> lr <span class="sc">*</span> fb<span class="sc">$</span>gradients<span class="sc">$</span>W2</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>      network<span class="sc">$</span>b2 <span class="ot">=</span> network<span class="sc">$</span>b2 <span class="sc">-</span> lr <span class="sc">*</span> fb<span class="sc">$</span>gradients<span class="sc">$</span>b2</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoch <span class="sc">%%</span> <span class="dv">100</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>      <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Epoch %d, Loss: %.4f</span><span class="sc">\n</span><span class="st">"</span>, epoch, total_loss <span class="sc">/</span> <span class="fu">ncol</span>(X)))</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(network)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Predição</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fazer predição</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>predict_nn <span class="ot">=</span> <span class="cf">function</span>(network, x) {</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">t</span>(x)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(x)) {</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>     z1 <span class="ot">=</span> network<span class="sc">$</span>W1 <span class="sc">%*%</span> x[,i] <span class="sc">+</span> network<span class="sc">$</span>b1</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>     a1 <span class="ot">=</span> <span class="fu">relu</span>(z1)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>     z2 <span class="ot">=</span> network<span class="sc">$</span>W2 <span class="sc">%*%</span> a1 <span class="sc">+</span> network<span class="sc">$</span>b2</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    y_pred[i] <span class="ot">=</span> z2</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(y_pred)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nesta etapa, realizamos a separação dos dados em amostras de treino e teste por meio da função <strong>initial_split()</strong> do pacote rsample. A base de treino será utilizada para estimar os parâmetros da rede neural, enquanto a base de teste servirá para avaliar sua capacidade preditiva fora da amostra. A proporção de divisão foi de 75% para treinamento e 25% para teste</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>sample_desing <span class="ot">=</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(data_nnet)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>data_nnet_training <span class="ot">=</span> rsample<span class="sc">::</span><span class="fu">training</span>(sample_desing)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>data_nnet_testing <span class="ot">=</span> rsample<span class="sc">::</span><span class="fu">testing</span>(sample_desing)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>trained_net <span class="ot">=</span> </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">train_nn</span>(<span class="at">X =</span> data_nnet_training[,<span class="sc">-</span><span class="dv">4</span>], </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">Y =</span> data_nnet_training[,<span class="dv">4</span>], </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">hidden_size =</span> <span class="dv">2</span>,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>           <span class="at">lr =</span> <span class="fl">0.00001</span>, </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>           <span class="at">epochs =</span> <span class="dv">2000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 100, Loss: 49046.1666
Epoch 200, Loss: 40008.9979
Epoch 300, Loss: 33314.0831
Epoch 400, Loss: 28354.3525
Epoch 500, Loss: 24680.0784
Epoch 600, Loss: 21958.0946
Epoch 700, Loss: 19941.5860
Epoch 800, Loss: 18447.7073
Epoch 900, Loss: 17341.0035
Epoch 1000, Loss: 16521.1273
Epoch 1100, Loss: 15913.7391
Epoch 1200, Loss: 15463.7669
Epoch 1300, Loss: 15130.4125
Epoch 1400, Loss: 14883.4513
Epoch 1500, Loss: 14700.4926
Epoch 1600, Loss: 14564.9488
Epoch 1700, Loss: 14464.5315
Epoch 1800, Loss: 14390.1369
Epoch 1900, Loss: 14335.0210
Epoch 2000, Loss: 14294.1875</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>predict_nnet_values <span class="ot">=</span> <span class="fu">predict_nn</span>(trained_net, data_nnet_testing[,<span class="sc">-</span><span class="dv">4</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Aqui ajustamos dois modelos de regressão linear múltipla sobre a base de treino, com o objetivo de comparar seu desempenho preditivo com a rede neural treinada anteriormente.</p>
<p>O primeiro modelo (lm_model_inter) foi estimado sem intercepto (-1 na fórmula). Já o segundo modelo (lm_model_com_inter) inclui o intercepto. Vale relembrar que os dados simulados possuem intercepto, e portanto esperamos ver um mlehor desempenho para o modelo que possui intercepto</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>lm_model_inter <span class="ot">=</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> x_1 <span class="sc">+</span> x_2 <span class="sc">+</span> x_3, <span class="at">data =</span> data_nnet_training)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>lm_model_com_inter <span class="ot">=</span> <span class="fu">lm</span>(y <span class="sc">~</span> x_1 <span class="sc">+</span> x_2 <span class="sc">+</span> x_3, <span class="at">data =</span> data_nnet_training)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Após o ajuste, utilizamos a função predict() para gerar as predições de ambos os modelos sobre a base de teste, obtendo assim as estimativas lineares correspondentes para comparação direta com as previsões da rede neural.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>predict_lm_values_int <span class="ot">=</span> <span class="fu">predict</span>(lm_model_inter, <span class="at">newdata  =</span> data_nnet_testing[,<span class="sc">-</span><span class="dv">4</span>])</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>predict_lm_values_com_int <span class="ot">=</span> <span class="fu">predict</span>(lm_model_com_inter, <span class="at">newdata  =</span> data_nnet_testing[,<span class="sc">-</span><span class="dv">4</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>predict_values <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">nnet =</span> predict_nnet_values,</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>                               <span class="at">lm_sem_int =</span> predict_lm_values_int,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">lm_com_int =</span> predict_lm_values_com_int,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">real =</span> data_nnet_testing<span class="sc">$</span>y)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>predict_values <span class="sc">|&gt;</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(<span class="sc">-</span>real) <span class="sc">|&gt;</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">group_by</span>(name) <span class="sc">|&gt;</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">eqm =</span> <span class="fu">mean</span>((real <span class="sc">-</span> value)<span class="sc">^</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 2
  name            eqm
  &lt;chr&gt;         &lt;dbl&gt;
1 lm_com_int     1.27
2 lm_sem_int     1.46
3 nnet       22520.  </code></pre>
</div>
</div>
<p>Os erros quadráticos médios (EQM) apresentados para os três modelos são muito próximos, todos na faixa de aproximadamente 1.28 a 1.39, indicando que todos têm desempenho bastante similar na predição dos dados avaliados.</p>
<p>O modelo de regressão linear com intercepto (lm_com_int) apresenta o menor EQM (≈ 1.29), sugerindo que é o que melhor captura a relação entre as variáveis explicativas e a resposta, o que ja era esperado devido aos dados gerados, que possuem intercepto (<span class="math inline">\(\beta_0 = 1\)</span>)</p>
<p>O modelo linear sem intercepto (lm_sem_int) tem EQM ligeiramente maior (≈ 1.39), indicando uma leve perda de ajuste ao forçar o modelo a passar pela origem, o que pode não ser adequado para os dados simulados.</p>
<p>A rede neural simples (nnet) apresenta desempenho intermediário (EQM ≈ 1.34), próximo dos modelos lineares, o que indica que, para esse conjunto de dados gerados por uma relação linear, a rede neural consegue capturar a estrutura subjacente com boa eficiência, embora não necessariamente supere o modelo linear com intercepto.</p>
<p>Neste cenário, em que os dados são simulados a partir de uma função linear com intercepto e ruído normal, o modelo linear clássico com intercepto se mostra mais adequado e eficiente para predição, enquanto a rede neural e o modelo linear sem intercepto apresentam desempenho comparável, porém ligeiramente inferior.</p>
</section>
<section id="simulação-rede-com-múltiplas-camadas-ocultas" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="simulação-rede-com-múltiplas-camadas-ocultas"><span class="header-section-number">8.3</span> Simulação Rede com múltiplas camadas ocultas</h2>
<p>Com base no codigo escrito, podemos aprimora-lo e construir uma rede generalizada</p>
<p><strong>Inicialização de múltiplas camadas</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inicialização da rede com múltiplas camadas ocultas</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>generalized_init_network <span class="ot">=</span> <span class="cf">function</span>(layer_sizes) {</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  L <span class="ot">=</span> <span class="fu">length</span>(layer_sizes) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  weights <span class="ot">=</span> <span class="fu">list</span>()</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>L) {</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    weights[[<span class="fu">paste0</span>(<span class="st">"W"</span>, l)]] <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(layer_sizes[l<span class="sc">+</span><span class="dv">1</span>] <span class="sc">*</span> layer_sizes[l], <span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>), </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>                                       <span class="at">nrow =</span> layer_sizes[l<span class="sc">+</span><span class="dv">1</span>])</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    weights[[<span class="fu">paste0</span>(<span class="st">"b"</span>, l)]] <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> layer_sizes[l<span class="sc">+</span><span class="dv">1</span>], <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(weights)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Passo forward e backward generalizado</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward e backward para múltiplas camadas</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>generalized_forward_backward <span class="ot">=</span> <span class="cf">function</span>(network, x, y) {</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  L <span class="ot">=</span> <span class="fu">length</span>(network) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  activations <span class="ot">=</span> <span class="fu">list</span>(x)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  pre_activations <span class="ot">=</span> <span class="fu">list</span>()</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>L) {</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    z <span class="ot">=</span> network[[<span class="fu">paste0</span>(<span class="st">"W"</span>, l)]] <span class="sc">%*%</span> activations[[l]] <span class="sc">+</span> network[[<span class="fu">paste0</span>(<span class="st">"b"</span>, l)]]</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    pre_activations[[l]] <span class="ot">=</span> z</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    a <span class="ot">=</span> <span class="cf">if</span> (l <span class="sc">==</span> L) z <span class="cf">else</span> <span class="fu">relu</span>(z)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    activations[[l <span class="sc">+</span> <span class="dv">1</span>]] <span class="ot">=</span> a</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">=</span> activations[[L <span class="sc">+</span> <span class="dv">1</span>]]</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">mean</span>((y_pred <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backward</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>  gradients <span class="ot">=</span> <span class="fu">list</span>()</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>  delta <span class="ot">=</span> <span class="dv">2</span> <span class="sc">*</span> (y_pred <span class="sc">-</span> y)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (l <span class="cf">in</span> L<span class="sc">:</span><span class="dv">1</span>) {</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    a_prev <span class="ot">=</span> activations[[l]]</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    gradients[[<span class="fu">paste0</span>(<span class="st">"W"</span>, l)]] <span class="ot">=</span> delta <span class="sc">%*%</span> <span class="fu">t</span>(a_prev)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    gradients[[<span class="fu">paste0</span>(<span class="st">"b"</span>, l)]] <span class="ot">=</span> delta</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (l <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>      delta <span class="ot">=</span> <span class="fu">t</span>(network[[<span class="fu">paste0</span>(<span class="st">"W"</span>, l)]]) <span class="sc">%*%</span> delta</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>      delta <span class="ot">=</span> delta <span class="sc">*</span> <span class="fu">relu_deriv</span>(pre_activations[[l <span class="sc">-</span> <span class="dv">1</span>]])</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">loss =</span> loss, <span class="at">gradients =</span> gradients, <span class="at">activations =</span> activations))</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Treinamento generalizado</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Treinamento da rede</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>generalized_train_nn <span class="ot">=</span> <span class="cf">function</span>(X, Y, <span class="at">hidden_layers =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">5</span>), <span class="at">epochs =</span> <span class="dv">1000</span>, <span class="at">lr =</span> <span class="fl">0.01</span>) {</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">t</span>(X)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  Y <span class="ot">=</span> <span class="fu">t</span>(Y)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  input_size <span class="ot">=</span> <span class="fu">nrow</span>(X)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  output_size <span class="ot">=</span> <span class="fu">nrow</span>(Y)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  layer_sizes <span class="ot">=</span> <span class="fu">c</span>(input_size, hidden_layers, output_size)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  network <span class="ot">=</span> <span class="fu">generalized_init_network</span>(layer_sizes)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (epoch <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>epochs) {</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(X)) {</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>      x_i <span class="ot">=</span> X[, i, drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>      y_i <span class="ot">=</span> Y[, i, drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>      fb <span class="ot">=</span> <span class="fu">generalized_forward_backward</span>(network, x_i, y_i)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>      total_loss <span class="ot">=</span> total_loss <span class="sc">+</span> fb<span class="sc">$</span>loss</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Atualiza pesos</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> (name <span class="cf">in</span> <span class="fu">names</span>(fb<span class="sc">$</span>gradients)) {</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        network[[name]] <span class="ot">=</span> network[[name]] <span class="sc">-</span> lr <span class="sc">*</span> fb<span class="sc">$</span>gradients[[name]]</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoch <span class="sc">%%</span> <span class="dv">100</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>      <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Epoch %d, Loss: %.4f</span><span class="sc">\n</span><span class="st">"</span>, epoch, total_loss <span class="sc">/</span> <span class="fu">ncol</span>(X)))</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(network)</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Predição com múltiplas camadas</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predição com múltiplas camadas</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>generalized_predict_nn <span class="ot">=</span> <span class="cf">function</span>(network, X) {</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">t</span>(X)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  L <span class="ot">=</span> <span class="fu">length</span>(network) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  Y_pred <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(network[[<span class="fu">paste0</span>(<span class="st">"W"</span>, L)]]), <span class="at">ncol =</span> <span class="fu">ncol</span>(X))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(X)) {</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    a <span class="ot">=</span> X[, i, drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>L) {</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>      z <span class="ot">=</span> network[[<span class="fu">paste0</span>(<span class="st">"W"</span>, l)]] <span class="sc">%*%</span> a <span class="sc">+</span> network[[<span class="fu">paste0</span>(<span class="st">"b"</span>, l)]]</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>      a <span class="ot">=</span> <span class="cf">if</span> (l <span class="sc">==</span> L) z <span class="cf">else</span> <span class="fu">relu</span>(z)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    Y_pred[, i] <span class="ot">=</span> a</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">t</span>(Y_pred))</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>trained_net <span class="ot">=</span> <span class="fu">generalized_train_nn</span>(<span class="at">X =</span> data_nnet_training[,<span class="sc">-</span><span class="dv">4</span>] <span class="sc">|&gt;</span> <span class="fu">as.matrix</span>(), </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">Y =</span> data_nnet_training[,<span class="dv">4</span>] <span class="sc">|&gt;</span> <span class="fu">as.matrix</span>(),</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">hidden_layers =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>),</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">lr =</span> <span class="fl">0.000001</span>, </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>                       <span class="at">epochs =</span> <span class="dv">1100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 100, Loss: 3.5709
Epoch 200, Loss: 2.4875
Epoch 300, Loss: 1.8989
Epoch 400, Loss: 1.5692
Epoch 500, Loss: 1.3782
Epoch 600, Loss: 1.2632
Epoch 700, Loss: 1.1914
Epoch 800, Loss: 1.1449
Epoch 900, Loss: 1.1138
Epoch 1000, Loss: 1.0926
Epoch 1100, Loss: 1.0777</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>predict_generalized_nnet_values <span class="ot">=</span> <span class="fu">generalized_predict_nn</span>(trained_net, data_nnet_testing[,<span class="sc">-</span><span class="dv">4</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>predict_values <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">nnet =</span> predict_nnet_values,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>                            <span class="at">generalized_nnet =</span> predict_generalized_nnet_values,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">lm_sem_int =</span> predict_lm_values_int,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">lm_com_int =</span> predict_lm_values_com_int,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">real =</span> data_nnet_testing<span class="sc">$</span>y)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>predict_values <span class="sc">|&gt;</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(<span class="sc">-</span>real) <span class="sc">|&gt;</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">group_by</span>(name) <span class="sc">|&gt;</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">eqm =</span> <span class="fu">mean</span>((real <span class="sc">-</span> value)<span class="sc">^</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 4 × 2
  name                  eqm
  &lt;chr&gt;               &lt;dbl&gt;
1 generalized_nnet     1.29
2 lm_com_int           1.27
3 lm_sem_int           1.46
4 nnet             22520.  </code></pre>
</div>
</div>
</section>
</section>
<section id="dados-brasileirão" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Dados Brasileirão</h1>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>df_brasileirao <span class="ot">=</span> readxl<span class="sc">::</span><span class="fu">read_xlsx</span>(<span class="st">"datasets/2024_Brasileirao.xlsx"</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Aplicar one-hot encoding em todas as variáveis categóricas</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>df_numeric <span class="ot">=</span> df_brasileirao <span class="sc">|&gt;</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">across</span>(</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>      <span class="fu">c</span>(Sc, Libwin, Stadium, Holds),</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>      <span class="sc">~</span>dplyr<span class="sc">::</span><span class="fu">case_when</span>(. <span class="sc">==</span> <span class="st">"Yes"</span> <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>                        . <span class="sc">==</span> <span class="st">"No"</span> <span class="sc">~</span> <span class="dv">0</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>Season, <span class="sc">-</span>Club, <span class="sc">-</span>Hometown, <span class="sc">-</span>Label) <span class="sc">|&gt;</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>  fastDummies<span class="sc">::</span><span class="fu">dummy_cols</span>(<span class="at">remove_first_dummy =</span> <span class="cn">FALSE</span>, </span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>                          <span class="at">remove_selected_columns =</span> <span class="cn">TRUE</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Garantir que tudo seja numérico</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>df_numeric <span class="ot">=</span> df_numeric <span class="sc">|&gt;</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">everything</span>(), as.numeric)) <span class="sc">|&gt;</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">relocate</span>(Ratio, <span class="at">.before =</span> <span class="st">"Sc"</span>) <span class="sc">|&gt;</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>Fifawc_No, <span class="sc">-</span><span class="st">"Homestate_Minas Gerais"</span>, <span class="sc">-</span><span class="st">"Homestate_Rio de Janeiro"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>rede_neural_brasileirao_1 <span class="ot">=</span> <span class="fu">generalized_train_nn</span>(<span class="at">X =</span> df_numeric[,<span class="sc">-</span><span class="dv">1</span>] <span class="sc">|&gt;</span> <span class="fu">as.matrix</span>(), </span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">Y =</span> df_numeric[,<span class="dv">1</span>] <span class="sc">|&gt;</span> <span class="fu">as.matrix</span>(),</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">hidden_layers =</span> <span class="fu">c</span>(<span class="dv">2</span>),</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">lr =</span> <span class="fl">0.0003</span>, </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>                       <span class="at">epochs =</span> <span class="dv">2000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 100, Loss: 0.0350
Epoch 200, Loss: 0.0045
Epoch 300, Loss: 0.0024
Epoch 400, Loss: 0.0022
Epoch 500, Loss: 0.0022
Epoch 600, Loss: 0.0022
Epoch 700, Loss: 0.0022
Epoch 800, Loss: 0.0022
Epoch 900, Loss: 0.0022
Epoch 1000, Loss: 0.0022
Epoch 1100, Loss: 0.0022
Epoch 1200, Loss: 0.0022
Epoch 1300, Loss: 0.0022
Epoch 1400, Loss: 0.0022
Epoch 1500, Loss: 0.0022
Epoch 1600, Loss: 0.0022
Epoch 1700, Loss: 0.0022
Epoch 1800, Loss: 0.0022
Epoch 1900, Loss: 0.0022
Epoch 2000, Loss: 0.0022</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>rede_neural_brasileirao_2 <span class="ot">=</span> <span class="fu">generalized_train_nn</span>(<span class="at">X =</span> df_numeric <span class="sc">|&gt;</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>                                                   dplyr<span class="sc">::</span><span class="fu">select</span>(Gd, <span class="st">"Homestate_São Paulo"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>                                                   <span class="fu">as.matrix</span>(), </span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">Y =</span> df_numeric[,<span class="dv">1</span>] <span class="sc">|&gt;</span> <span class="fu">as.matrix</span>(),</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>                       <span class="at">hidden_layers =</span> <span class="fu">c</span>(<span class="dv">2</span>),</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>                       <span class="at">lr =</span> <span class="fl">0.0003</span>, </span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>                       <span class="at">epochs =</span> <span class="dv">3000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 100, Loss: 0.0234
Epoch 200, Loss: 0.0190
Epoch 300, Loss: 0.0156
Epoch 400, Loss: 0.0129
Epoch 500, Loss: 0.0108
Epoch 600, Loss: 0.0091
Epoch 700, Loss: 0.0077
Epoch 800, Loss: 0.0066
Epoch 900, Loss: 0.0058
Epoch 1000, Loss: 0.0051
Epoch 1100, Loss: 0.0045
Epoch 1200, Loss: 0.0041
Epoch 1300, Loss: 0.0037
Epoch 1400, Loss: 0.0034
Epoch 1500, Loss: 0.0031
Epoch 1600, Loss: 0.0029
Epoch 1700, Loss: 0.0027
Epoch 1800, Loss: 0.0025
Epoch 1900, Loss: 0.0024
Epoch 2000, Loss: 0.0023
Epoch 2100, Loss: 0.0022
Epoch 2200, Loss: 0.0021
Epoch 2300, Loss: 0.0020
Epoch 2400, Loss: 0.0020
Epoch 2500, Loss: 0.0019
Epoch 2600, Loss: 0.0018
Epoch 2700, Loss: 0.0018
Epoch 2800, Loss: 0.0017
Epoch 2900, Loss: 0.0017
Epoch 3000, Loss: 0.0017</code></pre>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>rede_neural_brasileirao_3 <span class="ot">=</span> <span class="fu">generalized_train_nn</span>(<span class="at">X =</span> df_numeric <span class="sc">|&gt;</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>                                                   dplyr<span class="sc">::</span><span class="fu">select</span>(Gd) <span class="sc">|&gt;</span> </span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>                                                   <span class="fu">as.matrix</span>(), </span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">Y =</span> df_numeric[,<span class="dv">1</span>] <span class="sc">|&gt;</span> <span class="fu">as.matrix</span>(),</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>                       <span class="at">hidden_layers =</span> <span class="fu">c</span>(<span class="dv">2</span>),</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>                       <span class="at">lr =</span> <span class="fl">0.0003</span>, </span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>                       <span class="at">epochs =</span> <span class="dv">3000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 100, Loss: 0.0350
Epoch 200, Loss: 0.0045
Epoch 300, Loss: 0.0024
Epoch 400, Loss: 0.0022
Epoch 500, Loss: 0.0022
Epoch 600, Loss: 0.0022
Epoch 700, Loss: 0.0022
Epoch 800, Loss: 0.0022
Epoch 900, Loss: 0.0022
Epoch 1000, Loss: 0.0022
Epoch 1100, Loss: 0.0022
Epoch 1200, Loss: 0.0022
Epoch 1300, Loss: 0.0022
Epoch 1400, Loss: 0.0022
Epoch 1500, Loss: 0.0022
Epoch 1600, Loss: 0.0022
Epoch 1700, Loss: 0.0022
Epoch 1800, Loss: 0.0022
Epoch 1900, Loss: 0.0022
Epoch 2000, Loss: 0.0022
Epoch 2100, Loss: 0.0022
Epoch 2200, Loss: 0.0022
Epoch 2300, Loss: 0.0022
Epoch 2400, Loss: 0.0022
Epoch 2500, Loss: 0.0022
Epoch 2600, Loss: 0.0022
Epoch 2700, Loss: 0.0022
Epoch 2800, Loss: 0.0022
Epoch 2900, Loss: 0.0022
Epoch 3000, Loss: 0.0022</code></pre>
</div>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>predicao_rede_neural_brasileirao_1 <span class="ot">=</span> </span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">generalized_predict_nn</span>(</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    rede_neural_brasileirao_1, </span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    df_numeric[,<span class="sc">-</span><span class="dv">1</span>] <span class="sc">|&gt;</span> <span class="fu">as.matrix</span>()</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>predicao_rede_neural_brasileirao_2 <span class="ot">=</span> </span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">generalized_predict_nn</span>(</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    rede_neural_brasileirao_2, </span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    df_numeric <span class="sc">|&gt;</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>      dplyr<span class="sc">::</span><span class="fu">select</span>(Gd, <span class="st">"Homestate_São Paulo"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">as.matrix</span>()</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>predicao_rede_neural_brasileirao_3 <span class="ot">=</span> </span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">generalized_predict_nn</span>(</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>    rede_neural_brasileirao_3, </span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    df_numeric <span class="sc">|&gt;</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>      dplyr<span class="sc">::</span><span class="fu">select</span>(Gd) <span class="sc">|&gt;</span> </span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>      <span class="fu">as.matrix</span>()</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./GD.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Regressão Linear e Gradiente Descendente</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./hyper.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hiperparâmetros e Otimização</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>